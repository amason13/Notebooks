{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the PyTorch Seedlings Exercise\n",
    "\n",
    "This exercise will cover some key concepts in visual machine learning, including:\n",
    "\n",
    "* The preparation of image datasets\n",
    "* The construction of neural net models for image tasks\n",
    "* The use of *transfer learning,* a method for re-using previously trained models for new tasks\n",
    "\n",
    "## Notes on Using This Notebook\n",
    "\n",
    "* Code will be provided for boilerplate tasks; in other places, you will need to fill in code to complete the exercise. Cells you need to fill in will be flagged with the **Exercise** heading.\n",
    "* The code cells are, in general, meant to be run in order. If you think a code cell should be working, but it isn't, verify that all previous cells were run - the cell you're having trouble with may depend on a variable or file that is created in a previous cell.\n",
    "* Class names and other text normally meant for consumption by a computer will be rendered in a `monospace font`. This will hopefully reduce confusion between, e.g., the word \"dataset\" referring to the concept of a cohesive body of data, and the class name `Dataset` referring to the related PyTorch class.\n",
    "\n",
    "### Do This Now:\n",
    "\n",
    "The cell below downloads and unzips the dataset we'll be using for this exercise. The dataset is 1.8GB, so **please uncomment and execute the following code cell now** to get the process started. (The commented lines are there to prevent the download triggering accidentally, so you may wish to replace them afterward.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -0 https://s3-us-west-1.amazonaws.com/pytorch-course-datasets/plant-seedlings-classification.zip > seedlings.zip\n",
    "!unzip seedlings.zip\n",
    "!unzip train.zip\n",
    "!unzip test.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This exercise is based on the Kaggle competition, [Plant Seedlings Classification](https://www.kaggle.com/c/plant-seedlings-classification/overview). The goal is to create a neural net that can accurately classify newly sprouted plants as belonging to a particular species. Twelve species are represented in the training data, six crop plants and six undesirable weed plants.\n",
    "\n",
    "### The Training Dataset\n",
    "\n",
    "The training dataset is a set of almost 5000 image files, each depicting a seedling, sorted into folders labeled with the correct species name of each plant of interest:\n",
    "\n",
    "```\n",
    "train\n",
    "  \\--Black-grass\n",
    "  |    \\--0050f38b3.png\n",
    "  |    \\--0183fdf68.png\n",
    "  |    ...\n",
    "  \\--Charlock\n",
    "       \\--022179d65.png\n",
    "       \\--02c95e601.png\n",
    "       ...\n",
    "```\n",
    "\n",
    "We will train and validate our dataset with this data.\n",
    "\n",
    "### Multiple Iterations\n",
    "\n",
    "We'll show two approaches - one simpler, one more advanced. The simpler one will employ a simple model that we will train from scratch. The second approach will involve *transfer learning,* and will involve doing some domain-specific learning on an existing, pre-trained model.\n",
    "\n",
    "Don't forget that even if you want to jump ahead to the advanced exercise, it may depend on code executed in earlier cells.\n",
    "\n",
    "### The Final Step\n",
    "\n",
    "The *test* dataset is a separate, unlabeled set of images. The final step in today's exercise will be to use your model to classify the unlabeled images. You will export your predictions as a CSV file and upload them to the Kaggle site to receive a final accuracy score.\n",
    "\n",
    "## The First Iteration: Building from Scratch\n",
    "\n",
    "Let's Get Started! The code cell below contains imports we'll need; please execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Your Training Dataset\n",
    "\n",
    "In order for our images to be consumed by a model, it helps if they are regularized in some way. The function below resizes and crops the images to squares of a specified size. The random flip step is there to manage potential bias in the data. In this case, imagine how your model might be skewed if many photos in the dataset were all taken from the same angle in similar lighting, but new data presented for inference was created under different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(target_size=100, normalize=False):\n",
    "    t = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.RandomCrop(target_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    if normalize: # for imagenet-trained models specifically\n",
    "        t = transforms.Compose([\n",
    "            t,\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction above, the training data is a set of images, divided into folders, with each folder named for the image's class. There are twelve classes.\n",
    "\n",
    "This is a common enough arrangement that PyTorch (through the torchvision library) has an `ImageFolder` class that will build a PyTorch `Dataset` object for you from this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has:\n",
      "  4750 elements\n",
      "  12 classes\n"
     ]
    }
   ],
   "source": [
    "full_dataset = torchvision.datasets.ImageFolder('train', transform=get_transforms())\n",
    "print('This dataset has:')\n",
    "print('  {} elements'.format(len(full_dataset)))\n",
    "print('  {} classes'.format(len(full_dataset.classes)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a best practice to set aside part of your labeled data for validation. This guards against *overfitting.* The main symptom of overfitting is that a model seems to perform well in training, but does poorly when presented with new data. This happens when the model learns the dataset a little too well, and doesn't develop general rules for dealing with similar inputs. (Qualitatively, this can be compared with a child who has learned multiplication tables by rote up to 10x10, but hasn't learned a rule to multiply 13 x 16.) It often means that the model is overspecified with respect to the data - that is, that the parameter space of the model is large enough to form a map of the individual inputs to specific outputs.\n",
    "\n",
    "On the other hand, if your model performs just as accurately on the validation dataset as on the training dataset, that's a positive sign that it's learning as intended.\n",
    "\n",
    "Here, we use `torch.utils.data.random_split()` to extract training and validation sets with an 80/20 split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains 3800 elements\n",
      "Validation dataset contains 950 elements\n"
     ]
    }
   ],
   "source": [
    "train_len = int(0.8 * len(full_dataset))\n",
    "validate_len = len(full_dataset) - train_len\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(full_dataset, (train_len, validate_len))\n",
    "print('Training dataset contains {} elements'.format(len(train_dataset)))\n",
    "print('Validation dataset contains {} elements'.format(len(validate_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually convenient to package a `Dataset` in a `DataLoader`. When you're writing your own `Dataset` object, all you have to do is report the number of elements in the set, and return elements (with their labels, if needed) by index. The `DataLoader` handles everything else: Batching, shuffling, multi-threading I/O, sampling, and more. The `DataLoader` is the most common interface for offering data to a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Model That Might Work\n",
    "\n",
    "An earlier tutorial in this series, which made a classfier for CIFAR-10 images, used a variant of the LeNet-5 architecture, adapted for 3-channel color and larger images:\n",
    "\n",
    "```\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "The `__init__()` method defines two convolutional layers and three linear layers. Here's a quick review of what the parameters mean:\n",
    "\n",
    "* `conv1` is meant to take input with `3` channels (corresponding to the three color channels), and produce an output of `6` feature activation maps, with a detection window of `5` pixels square (it's kernel size). You can think of this layer as scanning the input image and looking for features it recognizes.\n",
    "* `conv2` takes output with `6` channels (corresponding to the 6 features detected by `conv1`), produces output for 16 features, and also employs a `5`-pixel window. You can think of this layer as composing the features detected by `conv1` into larger features.\n",
    "* `fc1` and `fc2` perform further processing on the output of the convnet layers.\n",
    "* `fc3` gives our final output, a vector of `10` elements. These are floating point numbers that relate to the model's confidence that the input belongs to a particular class.\n",
    "\n",
    "The `forward()` method composes these layers and some important functions into a computation graph that takes in a 3x32x32 tensor representing a 3-color image, and . Here's how the data flows through the graph:\n",
    "\n",
    "| Stage | Tensor Shape | Notes |\n",
    "| --- | --- | --- |\n",
    "| input | 3 x 32 x 32 | 32x32 image with 3 color channels |\n",
    "| conv1 | 6 x 28 x 28 | 6 features; spatial map reduced from 32 to 28 due to kernel size |\n",
    "| pooling | 6 x 14 x 14 | every 2 x 2 group of the map elements is reduced to a single element, which takes on the max value of its parent elements |\n",
    "| conv2 | 16 x 10 x 10 | 16 features; spatial map reduced from 14 to 10 due to kernel size |\n",
    "| pooling | 16 x 5 x 5 | as above, reducing resolution of the spatial map |\n",
    "| reshape | 1 x 400 | same data as the 3D tensor in the previous step, but flattened to a vector (400 = 16 x 5 x 5) |\n",
    "| fc1 | 1 x 120 | |\n",
    "| fc2 | 1 x 84 | |\n",
    "| output | 1 x 10 | 10 classes of data |\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Below is a skeleton version of the image classifier above, with most the parameters removed. (The 3-color input stays the same, and the `12` for the number of output classes has also been filled in.) **How would you fill in the values to make this work for our 100x100 seedling images?** Don't forget that some values are related, such as the output features of `conv1` and the input features of `conv2`. Some values are related to your input size as well, such as the input width of `fc1`.\n",
    "\n",
    "Things to think about and experiment with:\n",
    "\n",
    "**For the convolutional layers:** Does this model work using the same number of features (6 and 16) as before? Is there any advantage to altering the kernel size?\n",
    "\n",
    "Convolutional layers can also specify a *stride length:* A stride length of 1 means the kernel scans every possible position, a stride of 2 means it scans every other position, 3 means it scans every 3rd, and so on. If you enlarge the kernel, is there an advantage in setting a stride length?\n",
    "\n",
    "**For the linear layers:** Do the original input widths of the linear layers still work? (Hint: How does `fc1` respond to the new the 3x100x100 input size?) Can the intermediate values be left as-is, or is there benefit to changing them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class SeedlingModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeedlingModelV1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ?, ?)\n",
    "        self.conv2 = nn.Conv2d(?, ?, ?)\n",
    "        self.fc1 = nn.Linear(?, ?)\n",
    "        self.fc2 = nn.Linear(?, ?)\n",
    "        self.fc3 = nn.Linear(?, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), ?)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), ?)\n",
    "        x = x.view(-1, ?)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeedlingModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeedlingModelV1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 22 * 22, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be valuable to check your updated model architecture with the code in the next cell. If any of your layers are mismatched, you should get an error. This code:\n",
    "\n",
    "* instantiates the model\n",
    "* extracts an instance from the dataset\n",
    "* feeds the instance to the model for processing\n",
    "\n",
    "*(NB: The `torch.unsqueeze()` call is there because `forward()` actually expects a batch of tensors. Here, we have added a dimension at the beginning of our lone tensor to create a batch of 1.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0800, -0.1151, -0.1177, -0.0328,  0.0209,  0.0870,  0.0102,  0.0394,\n",
      "         -0.0764,  0.0479,  0.0628,  0.0336]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = SeedlingModelV1()\n",
    "image, label = train_dataset.__getitem__(0)\n",
    "output = model.forward(torch.unsqueeze(image, 0))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "First, we'll define a few constants, including the learning hyperparameters. It can be convenient to have these parameters defined in one place, or specified on the command line, to make it easy to tune them as you're shaking out your training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20 # number of passes over the training dataset\n",
    "LR = 0.01 # learning rate\n",
    "MOMENTUM = 0.5 # for SGD\n",
    "\n",
    "BATCH_SIZE = 4 # number of instances per batch served by dataloader\n",
    "NUM_WORKERS = 2 # number of I/O threads used by dataloader\n",
    "\n",
    "MODEL_DIR = 'models' # save models here\n",
    "MODEL_SAVEFILE = 'seedling'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll need to create that folder for our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we train and validate the model, we'll want informative logging so that we know what's going on, and roughly how long it should take. Also, it's a good practice to save the model when it reaches a new accuracy peak, so we'll create a helper for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlog(msg):\n",
    "    print('{}   {}'.format(time.asctime(), msg))\n",
    "\n",
    "    \n",
    "def save_model(model, epoch):\n",
    "    tlog('Saving model')\n",
    "    savefile = \"{}-e{}-{}.pt\".format(MODEL_SAVEFILE, epoch, int(time.time()))\n",
    "    path = os.path.join(MODEL_DIR, savefile)\n",
    "    # recommended way from https://pytorch.org/docs/stable/notes/serialization.html\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "    return savefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can, we'd like to run this on GPU. Below, we'll check for the presence of a CUDA-compatible device and get a handle to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available - running on CPU.\n",
      "If you are running this notebook in Colab, go to the Runtime menu and select \"Change runtime type\" to switch to GPU.\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU not available - running on CPU.\\nIf you are running this notebook in Colab, go to the Runtime menu and select \"Change runtime type\" to switch to GPU.')\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just to make sure we're starting from *tabula rasa* (and for review), let's recreate the key components of our process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = torchvision.datasets.ImageFolder('train', transform=get_transforms())\n",
    "train_len = int(0.8 * len(full_dataset))\n",
    "validate_len = len(full_dataset) - train_len\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(full_dataset, (train_len, validate_len))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset, batch_size=1)\n",
    "\n",
    "model = SeedlingModelV1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have an untrained model in `model`, our data ready to consume from `train_loader` and `validate_loader`, and a `device` selected. It's time to train!\n",
    "\n",
    "The structure of this training loop should be familiar from previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=N_EPOCHS):\n",
    "    tlog('Training the model...')\n",
    "    \n",
    "    best_accuracy = 0. # determines whether we save a copy of the model\n",
    "    saved_model_filename = None\n",
    "    \n",
    "    model = model.to(device) # move to GPU if available\n",
    "    loss_fn = nn.CrossEntropyLoss() # combines nn.LogSoftmax() and nn.NLLLoss() for classification tasks\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tlog('BEGIN EPOCH {} of {}'.format(epoch + 1, epochs))\n",
    "        running_loss = 0. # bookkeeping\n",
    "        \n",
    "        tlog('Train:')\n",
    "        for i, data in enumerate(train_loader):\n",
    "            instances, labels = data[0], data[1]\n",
    "            instances, labels = instances.to(device), labels.to(device) # move to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            guesses = model(instances)\n",
    "            loss = loss_fn(guesses, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 200 == 0: # log every 200 batches\n",
    "                tlog('  batch {}   avg loss: {}'.format(i + 1, running_loss / (200)))\n",
    "                running_loss = 0.\n",
    "        \n",
    "        tlog('Validate:')\n",
    "        with torch.no_grad(): # no need to do expensive gradient computation for validation\n",
    "            total_loss = 0.\n",
    "            correct = 0\n",
    "            \n",
    "            for i, data in enumerate(validate_loader):\n",
    "                instance, label = data[0], data[1]\n",
    "                instance, label = instance.to(device), label.to(device) # move to GPU if available\n",
    "                \n",
    "                guess = model(instance)\n",
    "                loss = loss_fn(guess, label)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                prediction = torch.argmax(guess, 1)\n",
    "                if prediction.item() == label.item(): # assuming batch size of 1\n",
    "                    correct += 1\n",
    "\n",
    "            avg_loss = total_loss / len(validate_loader)\n",
    "            accuracy = correct / len(validate_loader)\n",
    "            tlog('  Avg loss for epoch: {}   accuracy: {}'.format(avg_loss, accuracy))\n",
    "            \n",
    "            if accuracy >= best_accuracy:\n",
    "                tlog( '  New accuracy peak, saving model')\n",
    "                best_accuracy = accuracy\n",
    "                saved_model_filename = save_model(model, epoch + 1)\n",
    "                \n",
    "    return (saved_model_filename, best_accuracy)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the training loop, you should see the loss decreasing and accuracy increasing more-or-less monotonically, both for training and for validation. You should also see the average per-instance loss values roughly similar for validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  8 16:26:20 2019   Training the model...\n",
      "Wed May  8 16:26:20 2019   BEGIN EPOCH 1 of 20\n",
      "Wed May  8 16:26:20 2019   Train:\n",
      "Wed May  8 16:26:28 2019     batch 200   avg loss: 2.4408892291784285\n",
      "Wed May  8 16:26:35 2019     batch 400   avg loss: 2.424518073797226\n",
      "Wed May  8 16:26:43 2019     batch 600   avg loss: 2.408646847009659\n",
      "Wed May  8 16:26:51 2019     batch 800   avg loss: 2.374582479596138\n",
      "Wed May  8 16:26:56 2019   Validate:\n",
      "Wed May  8 16:27:07 2019     Avg loss for epoch: 2.347108101970271   accuracy: 0.15368421052631578\n",
      "Wed May  8 16:27:07 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:27:07 2019   Saving model\n",
      "Wed May  8 16:27:07 2019   BEGIN EPOCH 2 of 20\n",
      "Wed May  8 16:27:07 2019   Train:\n",
      "Wed May  8 16:27:15 2019     batch 200   avg loss: 2.2742838722467424\n",
      "Wed May  8 16:27:22 2019     batch 400   avg loss: 2.344126656651497\n",
      "Wed May  8 16:27:29 2019     batch 600   avg loss: 2.2710443186759948\n",
      "Wed May  8 16:27:37 2019     batch 800   avg loss: 2.0749044781923294\n",
      "Wed May  8 16:27:42 2019   Validate:\n",
      "Wed May  8 16:27:54 2019     Avg loss for epoch: 2.451831903583125   accuracy: 0.13894736842105262\n",
      "Wed May  8 16:27:54 2019   BEGIN EPOCH 3 of 20\n",
      "Wed May  8 16:27:54 2019   Train:\n",
      "Wed May  8 16:28:01 2019     batch 200   avg loss: 2.113621763586998\n",
      "Wed May  8 16:28:08 2019     batch 400   avg loss: 1.8684742558002472\n",
      "Wed May  8 16:28:15 2019     batch 600   avg loss: 1.827071306705475\n",
      "Wed May  8 16:28:21 2019     batch 800   avg loss: 1.7700883051753045\n",
      "Wed May  8 16:28:26 2019   Validate:\n",
      "Wed May  8 16:28:38 2019     Avg loss for epoch: 1.627887754565791   accuracy: 0.4284210526315789\n",
      "Wed May  8 16:28:38 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:28:38 2019   Saving model\n",
      "Wed May  8 16:28:38 2019   BEGIN EPOCH 4 of 20\n",
      "Wed May  8 16:28:38 2019   Train:\n",
      "Wed May  8 16:28:44 2019     batch 200   avg loss: 1.6991560715436935\n",
      "Wed May  8 16:28:51 2019     batch 400   avg loss: 1.7568521505594255\n",
      "Wed May  8 16:28:58 2019     batch 600   avg loss: 1.652943488061428\n",
      "Wed May  8 16:29:05 2019     batch 800   avg loss: 1.6220867547392845\n",
      "Wed May  8 16:29:10 2019   Validate:\n",
      "Wed May  8 16:29:21 2019     Avg loss for epoch: 1.5236397603938454   accuracy: 0.4421052631578947\n",
      "Wed May  8 16:29:21 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:29:21 2019   Saving model\n",
      "Wed May  8 16:29:21 2019   BEGIN EPOCH 5 of 20\n",
      "Wed May  8 16:29:21 2019   Train:\n",
      "Wed May  8 16:29:27 2019     batch 200   avg loss: 1.6002974092960358\n",
      "Wed May  8 16:29:34 2019     batch 400   avg loss: 1.5864313271641732\n",
      "Wed May  8 16:29:40 2019     batch 600   avg loss: 1.508686521947384\n",
      "Wed May  8 16:29:47 2019     batch 800   avg loss: 1.58517499178648\n",
      "Wed May  8 16:29:53 2019   Validate:\n",
      "Wed May  8 16:30:04 2019     Avg loss for epoch: 1.498597398808128   accuracy: 0.47789473684210526\n",
      "Wed May  8 16:30:04 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:30:04 2019   Saving model\n",
      "Wed May  8 16:30:04 2019   BEGIN EPOCH 6 of 20\n",
      "Wed May  8 16:30:04 2019   Train:\n",
      "Wed May  8 16:30:11 2019     batch 200   avg loss: 1.59445760846138\n",
      "Wed May  8 16:30:17 2019     batch 400   avg loss: 1.505426584482193\n",
      "Wed May  8 16:30:24 2019     batch 600   avg loss: 1.4480756175518037\n",
      "Wed May  8 16:30:30 2019     batch 800   avg loss: 1.3615400928258896\n",
      "Wed May  8 16:30:35 2019   Validate:\n",
      "Wed May  8 16:30:46 2019     Avg loss for epoch: 1.321356439841421   accuracy: 0.54\n",
      "Wed May  8 16:30:46 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:30:46 2019   Saving model\n",
      "Wed May  8 16:30:46 2019   BEGIN EPOCH 7 of 20\n",
      "Wed May  8 16:30:46 2019   Train:\n",
      "Wed May  8 16:30:53 2019     batch 200   avg loss: 1.2839977353811265\n",
      "Wed May  8 16:30:59 2019     batch 400   avg loss: 1.3492453074455262\n",
      "Wed May  8 16:31:06 2019     batch 600   avg loss: 1.272411361336708\n",
      "Wed May  8 16:31:13 2019     batch 800   avg loss: 1.2281244349479676\n",
      "Wed May  8 16:31:18 2019   Validate:\n",
      "Wed May  8 16:31:29 2019     Avg loss for epoch: 1.290636341320841   accuracy: 0.5463157894736842\n",
      "Wed May  8 16:31:29 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:31:29 2019   Saving model\n",
      "Wed May  8 16:31:29 2019   BEGIN EPOCH 8 of 20\n",
      "Wed May  8 16:31:29 2019   Train:\n",
      "Wed May  8 16:31:36 2019     batch 200   avg loss: 1.1423470723628997\n",
      "Wed May  8 16:31:44 2019     batch 400   avg loss: 1.1249913215637206\n",
      "Wed May  8 16:31:49 2019     batch 600   avg loss: 1.1779335302114486\n",
      "Wed May  8 16:31:56 2019     batch 800   avg loss: 1.1316154900193214\n",
      "Wed May  8 16:32:01 2019   Validate:\n",
      "Wed May  8 16:32:13 2019     Avg loss for epoch: 1.2154559062656602   accuracy: 0.58\n",
      "Wed May  8 16:32:13 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:32:13 2019   Saving model\n",
      "Wed May  8 16:32:13 2019   BEGIN EPOCH 9 of 20\n",
      "Wed May  8 16:32:13 2019   Train:\n",
      "Wed May  8 16:32:19 2019     batch 200   avg loss: 1.0333919289708138\n",
      "Wed May  8 16:32:26 2019     batch 400   avg loss: 0.9650550791621209\n",
      "Wed May  8 16:32:33 2019     batch 600   avg loss: 0.9689489534497261\n",
      "Wed May  8 16:32:40 2019     batch 800   avg loss: 1.0148777762055397\n",
      "Wed May  8 16:32:45 2019   Validate:\n",
      "Wed May  8 16:32:56 2019     Avg loss for epoch: 1.0747293870072616   accuracy: 0.64\n",
      "Wed May  8 16:32:56 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:32:56 2019   Saving model\n",
      "Wed May  8 16:32:56 2019   BEGIN EPOCH 10 of 20\n",
      "Wed May  8 16:32:56 2019   Train:\n",
      "Wed May  8 16:33:02 2019     batch 200   avg loss: 0.8370771273970604\n",
      "Wed May  8 16:33:09 2019     batch 400   avg loss: 0.8447713354229927\n",
      "Wed May  8 16:33:16 2019     batch 600   avg loss: 0.8928659784793854\n",
      "Wed May  8 16:33:23 2019     batch 800   avg loss: 0.9128765949606895\n",
      "Wed May  8 16:33:28 2019   Validate:\n",
      "Wed May  8 16:33:39 2019     Avg loss for epoch: 1.1400064219926533   accuracy: 0.6326315789473684\n",
      "Wed May  8 16:33:39 2019   BEGIN EPOCH 11 of 20\n",
      "Wed May  8 16:33:39 2019   Train:\n",
      "Wed May  8 16:33:46 2019     batch 200   avg loss: 0.774339632987976\n",
      "Wed May  8 16:33:53 2019     batch 400   avg loss: 0.8377624782919884\n",
      "Wed May  8 16:33:59 2019     batch 600   avg loss: 0.8333211326599121\n",
      "Wed May  8 16:34:06 2019     batch 800   avg loss: 0.740383083820343\n",
      "Wed May  8 16:34:11 2019   Validate:\n",
      "Wed May  8 16:34:22 2019     Avg loss for epoch: 1.0295210495748017   accuracy: 0.6726315789473685\n",
      "Wed May  8 16:34:22 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:34:22 2019   Saving model\n",
      "Wed May  8 16:34:22 2019   BEGIN EPOCH 12 of 20\n",
      "Wed May  8 16:34:22 2019   Train:\n",
      "Wed May  8 16:34:29 2019     batch 200   avg loss: 0.6974057322740554\n",
      "Wed May  8 16:34:35 2019     batch 400   avg loss: 0.7257398173213006\n",
      "Wed May  8 16:34:42 2019     batch 600   avg loss: 0.6940518665313721\n",
      "Wed May  8 16:34:49 2019     batch 800   avg loss: 0.6828791563212872\n",
      "Wed May  8 16:34:54 2019   Validate:\n",
      "Wed May  8 16:35:05 2019     Avg loss for epoch: 1.0169254569003456   accuracy: 0.6610526315789473\n",
      "Wed May  8 16:35:05 2019   BEGIN EPOCH 13 of 20\n",
      "Wed May  8 16:35:05 2019   Train:\n",
      "Wed May  8 16:35:13 2019     batch 200   avg loss: 0.622170245051384\n",
      "Wed May  8 16:35:19 2019     batch 400   avg loss: 0.6735652300715447\n",
      "Wed May  8 16:35:26 2019     batch 600   avg loss: 0.5965683928132057\n",
      "Wed May  8 16:35:32 2019     batch 800   avg loss: 0.605601963698864\n",
      "Wed May  8 16:35:37 2019   Validate:\n",
      "Wed May  8 16:35:48 2019     Avg loss for epoch: 0.9912108510418942   accuracy: 0.6757894736842105\n",
      "Wed May  8 16:35:48 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:35:48 2019   Saving model\n",
      "Wed May  8 16:35:48 2019   BEGIN EPOCH 14 of 20\n",
      "Wed May  8 16:35:48 2019   Train:\n",
      "Wed May  8 16:35:55 2019     batch 200   avg loss: 0.5429089841246605\n",
      "Wed May  8 16:36:02 2019     batch 400   avg loss: 0.5109513542056083\n",
      "Wed May  8 16:36:08 2019     batch 600   avg loss: 0.5947366496920585\n",
      "Wed May  8 16:36:16 2019     batch 800   avg loss: 0.5494335514307022\n",
      "Wed May  8 16:36:20 2019   Validate:\n",
      "Wed May  8 16:36:32 2019     Avg loss for epoch: 1.0640669830221878   accuracy: 0.6578947368421053\n",
      "Wed May  8 16:36:32 2019   BEGIN EPOCH 15 of 20\n",
      "Wed May  8 16:36:32 2019   Train:\n",
      "Wed May  8 16:36:40 2019     batch 200   avg loss: 0.46702732622623444\n",
      "Wed May  8 16:36:47 2019     batch 400   avg loss: 0.5184494234621525\n",
      "Wed May  8 16:36:54 2019     batch 600   avg loss: 0.39698001474142075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  8 16:37:01 2019     batch 800   avg loss: 0.6167449614405632\n",
      "Wed May  8 16:37:06 2019   Validate:\n",
      "Wed May  8 16:37:19 2019     Avg loss for epoch: 1.064957264850014   accuracy: 0.6789473684210526\n",
      "Wed May  8 16:37:19 2019     New accuracy peak, saving model\n",
      "Wed May  8 16:37:19 2019   Saving model\n",
      "Wed May  8 16:37:19 2019   BEGIN EPOCH 16 of 20\n",
      "Wed May  8 16:37:19 2019   Train:\n",
      "Wed May  8 16:37:27 2019     batch 200   avg loss: 0.43457256615161893\n",
      "Wed May  8 16:37:34 2019     batch 400   avg loss: 0.4400522553920746\n",
      "Wed May  8 16:37:42 2019     batch 600   avg loss: 0.47951856106519697\n",
      "Wed May  8 16:37:48 2019     batch 800   avg loss: 0.4267819634079933\n",
      "Wed May  8 16:37:53 2019   Validate:\n",
      "Wed May  8 16:38:05 2019     Avg loss for epoch: 1.0964412139591417   accuracy: 0.6631578947368421\n",
      "Wed May  8 16:38:05 2019   BEGIN EPOCH 17 of 20\n",
      "Wed May  8 16:38:05 2019   Train:\n",
      "Wed May  8 16:38:13 2019     batch 200   avg loss: 0.33576897144317625\n",
      "Wed May  8 16:38:20 2019     batch 400   avg loss: 0.35078449189662936\n",
      "Wed May  8 16:38:27 2019     batch 600   avg loss: 0.398800368309021\n",
      "Wed May  8 16:38:35 2019     batch 800   avg loss: 0.46140933573246\n",
      "Wed May  8 16:38:40 2019   Validate:\n",
      "Wed May  8 16:38:52 2019     Avg loss for epoch: 1.0818108227378445   accuracy: 0.6589473684210526\n",
      "Wed May  8 16:38:52 2019   BEGIN EPOCH 18 of 20\n",
      "Wed May  8 16:38:52 2019   Train:\n",
      "Wed May  8 16:38:59 2019     batch 200   avg loss: 0.3455215501785278\n",
      "Wed May  8 16:39:06 2019     batch 400   avg loss: 0.38941921800374985\n",
      "Wed May  8 16:39:13 2019     batch 600   avg loss: 0.3082797414064407\n",
      "Wed May  8 16:39:20 2019     batch 800   avg loss: 0.39483488380908965\n",
      "Wed May  8 16:39:25 2019   Validate:\n",
      "Wed May  8 16:39:36 2019     Avg loss for epoch: 1.0923044410504792   accuracy: 0.6673684210526316\n",
      "Wed May  8 16:39:36 2019   BEGIN EPOCH 19 of 20\n",
      "Wed May  8 16:39:36 2019   Train:\n",
      "Wed May  8 16:39:44 2019     batch 200   avg loss: 0.3263242170214653\n",
      "Wed May  8 16:39:50 2019     batch 400   avg loss: 0.29081919938325884\n",
      "Wed May  8 16:39:56 2019     batch 600   avg loss: 0.3650909107923508\n",
      "Wed May  8 16:40:03 2019     batch 800   avg loss: 0.3526556700468063\n",
      "Wed May  8 16:40:08 2019   Validate:\n",
      "Wed May  8 16:40:20 2019     Avg loss for epoch: 1.2246188326885825   accuracy: 0.6663157894736842\n",
      "Wed May  8 16:40:20 2019   BEGIN EPOCH 20 of 20\n",
      "Wed May  8 16:40:20 2019   Train:\n",
      "Wed May  8 16:40:26 2019     batch 200   avg loss: 0.30866678804159164\n",
      "Wed May  8 16:40:34 2019     batch 400   avg loss: 0.3257240104675293\n",
      "Wed May  8 16:40:40 2019     batch 600   avg loss: 0.28849061101675033\n",
      "Wed May  8 16:40:47 2019     batch 800   avg loss: 0.29668363809585574\n",
      "Wed May  8 16:40:52 2019   Validate:\n",
      "Wed May  8 16:41:03 2019     Avg loss for epoch: 1.1260002545306558   accuracy: 0.6789473684210526\n",
      "The best model is saved at seedling-e15-1557358639.pt with accuracy 0.6789473684210526\n"
     ]
    }
   ],
   "source": [
    "best_model_filename, accuracy  = train(model)\n",
    "print('The best model is saved at {} with accuracy {}'.format(best_model_filename, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "**What accuracy did you achieve?** Did the model converge (i.e., did the per-instance loss flatten out) in the number of epochs you ran? Was the loss during validation similar to the loss during training?\n",
    "\n",
    "**Was the learning stable?** Did loss continue to decrease and accuracy increase monotonically?\n",
    "\n",
    "**How could it improve?** Consider the many choices we've made up to this point, and their effect on the model architecture, the state of the data, and the execution of the training loop:\n",
    "\n",
    "* **Data:** We regularized the *shape* of the training data, but performed no other normalization. (For more information, see the discussion below on normalization of the color space.) Could the data be altered in some way that improves accuracy?\n",
    "* **Convnet Layers:** Convolutional layers make use of multiple important parameters. Would preformance be improved with a change to the number of input or output features, or the kernel size, or the stride length?\n",
    "\n",
    "If you feel your run could have been better, hypothesize about which of the above factors might affect it, and pick one or two to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_filename, accuracy  = train(model, epochs=5)\n",
    "print('The best model is saved at {} with accuracy {}'.format(best_model_filename, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SeedlingModelV1_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SeedlingModelV1_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 9)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 21 * 21, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.fc1.in_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model_1_1 = SeedlingModelV1_1()\n",
    "best_model_1_1, acc_1_1 = train(model_1_1)\n",
    "print('The best updated model is saved at {} with accuracy {}'.format(best_model_1_1, acc_1_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_model_params(model))\n",
    "print(count_model_params(model_1_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Second Iteration: Tuning an Existing Model\n",
    "\n",
    "Now, we'll look at a second technique: Making adjustments to a pre-trained model.\n",
    "\n",
    "The very best computer vision models can be large indeed. Here's a sampling of the parameter counts of some of the pre-trained models available with the `torchvision` library:\n",
    "\n",
    "| Model | Number of Parameters |\n",
    "| --- | --- |\n",
    "| SeedlingModelV1 (above) | 943,456 |\n",
    "| SqueezeNet 1.1 | 1,235,496 |\n",
    "| Resnet 50 | 25,557,032 |\n",
    "| Densenet-161 | 28,681,000 |\n",
    "| Alexnet | 61,100,840 |\n",
    "| VGG-16 | 138,357,544 |\n",
    "\n",
    "Training a model with tens of millions of parameters - or more! - can take a huge amount of time, even if you have access to hardware acceleration. The good news, as we covered in the earlier unit on transfer learning, is that you can leverage pre-trained models for your problem domain, and greatly reduce your training time.\n",
    "\n",
    "The pre-trained models available in `torchvision` are all trained against [ImageNet](http://www.image-net.org/about-overview) - a general-purpose set of over a million images drawn from the World Wide Web, categorized by their content into 1000 different categories. We'll be adapting one of these models to see if we can achieve better results while still only incurring a short cost for training time.\n",
    "\n",
    "### The Model\n",
    "\n",
    "### The Data\n",
    "\n",
    "* image size - resize or discard?\n",
    "* normalization\n",
    "\n",
    "### The Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = torchvision.models.densenet161(pretrained=True)\n",
    "r50 = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense.classifier = nn.Linear(in_features=2208, out_features=12, bias=True)\n",
    "r50.fc = nn.Linear(in_features=2048, out_features=12, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains 3800 elements\n",
      "Validation dataset contains 950 elements\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# NOTE: We are altering variable values from above\n",
    "full_dataset = torchvision.datasets.ImageFolder('train', transform=get_transforms(target_size=224, normalize=True))\n",
    "train_len = int(0.8 * len(full_dataset))\n",
    "validate_len = len(full_dataset) - train_len\n",
    "train_dataset, validate_dataset = torch.utils.data.random_split(full_dataset, (train_len, validate_len))\n",
    "print('Training dataset contains {} elements'.format(len(train_dataset)))\n",
    "print('Validation dataset contains {} elements'.format(len(validate_dataset)))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset)\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images[0].shape) # should be a 3-color image @ 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=N_EPOCHS):\n",
    "    tlog('Training the model...')\n",
    "    \n",
    "    best_accuracy = 0. # determines whether we save a copy of the model\n",
    "    saved_model_filename = None\n",
    "    \n",
    "    model = model.to(device) # move to GPU if available\n",
    "    loss_fn = nn.CrossEntropyLoss() # combines nn.LogSoftmax() and nn.NLLLoss() for classification tasks\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tlog('BEGIN EPOCH {} of {}'.format(epoch + 1, epochs))\n",
    "        running_loss = 0. # bookkeeping\n",
    "        \n",
    "        tlog('Train:')\n",
    "        for i, data in enumerate(train_loader):\n",
    "            instances, labels = data[0], data[1]\n",
    "            instances, labels = instances.to(device), labels.to(device) # move to GPU if available\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            guesses = model(instances)\n",
    "            loss = loss_fn(guesses, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 200 == 0: # log every 200 batches\n",
    "                tlog('  batch {}   avg loss: {}'.format(i + 1, running_loss / (200)))\n",
    "                running_loss = 0.\n",
    "        \n",
    "        tlog('Validate:')\n",
    "        with torch.no_grad(): # no need to do expensive gradient computation for validation\n",
    "            total_loss = 0.\n",
    "            correct = 0\n",
    "            \n",
    "            for i, data in enumerate(validate_loader):\n",
    "                instance, label = data[0], data[1]\n",
    "                instance, label = instance.to(device), label.to(device) # move to GPU if available\n",
    "                \n",
    "                guess = model(instance)\n",
    "                loss = loss_fn(guess, label)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                prediction = torch.argmax(guess, 1)\n",
    "                if prediction.item() == label.item(): # assuming batch size of 1\n",
    "                    correct += 1\n",
    "\n",
    "            avg_loss = total_loss / len(validate_loader)\n",
    "            accuracy = correct / len(validate_loader)\n",
    "            tlog('  Avg loss for epoch: {}   accuracy: {}'.format(avg_loss, accuracy))\n",
    "            \n",
    "            if accuracy >= best_accuracy:\n",
    "                tlog( '  New accuracy peak, saving model')\n",
    "                best_accuracy = accuracy\n",
    "                saved_model_filename = save_model(model, epoch + 1)\n",
    "                \n",
    "    return (saved_model_filename, best_accuracy)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May  8 17:51:30 2019   Training the model...\n",
      "Wed May  8 17:51:30 2019   BEGIN EPOCH 1 of 10\n",
      "Wed May  8 17:51:30 2019   Train:\n",
      "Wed May  8 17:55:32 2019     batch 200   avg loss: 1.960296801328659\n",
      "Wed May  8 17:59:25 2019     batch 400   avg loss: 1.3483356368541717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-41afb6dad025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train(model, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-192fd41fc85d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mguesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pyto/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1696\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m     )\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train(model, epochs=1)\n",
    "train(r50, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
