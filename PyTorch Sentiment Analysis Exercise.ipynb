{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Sentiment Analysis Exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWf9aWoHk-rT",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to the PyTorch Sentiment Analysis Exercise\n",
        "\n",
        "This exercise will cover some key concepts in natural language machine learning, including:\n",
        "\n",
        "* The preparation of text datasets\n",
        "* The construction of neural net models for natural language tasks\n",
        "* The use of *recurrent neural network layers,* which are commonly used in problems involving sequences, including natural language tasks.\n",
        "\n",
        "## Notes on Using This Notebook\n",
        "\n",
        "* Code will be provided for boilerplate tasks; in other places, you will need to fill in code to complete the exercise. Cells you need to fill in will be flagged with the **Exercise** heading.\n",
        "* The code cells are, in general, meant to be run in order. If you think a code cell should be working, but it isn't, verify that all previous cells were run - the cell you're having trouble with may depend on a variable or file that is created in a previous cell.\n",
        "* Class names and other text normally meant for consumption by a computer will be rendered in a `monospace font`. This will hopefully reduce confusion between, e.g., the word \"dataset\" referring to the concept of a cohesive body of data, and the class name `Dataset` referring to the related PyTorch class.\n",
        "\n",
        "### Do This Now:\n",
        "\n",
        "The cell below downloads and unzips the dataset we'll be using for this exercise. The dataset isn't huge, but it may take a minute to download, so **please uncomment and execute the following code cell now** to get the process started. (The commented lines are there to prevent the download triggering accidentally, so you may wish to replace them afterward.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p2iEyrC34qE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !curl -0 https://s3-us-west-1.amazonaws.com/pytorch-course-datasets/sentiment-analysis-on-movie-reviews.zip > reviews.zip\n",
        "# !unzip reviews.zip\n",
        "# !mkdir models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k83QPxu2lEJ1",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This exercise is based on the Kaggle competition, [Sentiment Analysis on Movie Reviews](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview). The goal is to accurately classify movie reviews based on how positively they speak of the movie being reviewed.\n",
        "\n",
        "### The Training Dataset\n",
        "\n",
        "Let's have a look at the input data. In `train.tsv`, you should see a file with four fields: A phrase ID, a sentence ID, a phrase, and a sentiment score. (The score runs from 0 to 4, with higher numbers indicating more positive sentiment.) You should see the same content in `test.tsv`, but without the scores.\n",
        "\n",
        "### The Approach\n",
        "\n",
        "There are many approaches to Natural Language Processing (NLP) using deep learning, and they will often include a variety of common neural network constructs, including fully-connected layers, recurrent neural networks (RNNs), convnets, embeddings, and dropouts. For this exercise, we'll be guiding you through constructing a simple model using word embeddings and RNNs, and pointing out directions for further enhancement and exploration.\n",
        "\n",
        "### The Final Step\n",
        "\n",
        "The test dataset is a separate, unlabeled collection of phrases from movie reviews. The final step in today's exercise will be to use your model to classify the unlabeled phrases. You will export your predictions to a file and upload them to the Kaggle site to receive a final accuracy score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XheP7WvtlK2x",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "In the cells below, we will:\n",
        "* ...import some necessary modules\n",
        "* ...define a few boilerplate helper functions\n",
        "* ...create a global device handle for our CPU or GPU device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qenT9rGd3_Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext import data       # NLP data utilities\n",
        "\n",
        "import random\n",
        "import time                      # timestamping messages and saved models\n",
        "import os                        # for filesystem methods"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7zmDkFp4O5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tlog(msg):\n",
        "    print('{}   {}'.format(time.asctime(), msg))\n",
        "\n",
        "def count_model_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def count_correct(guesses, labels):\n",
        "    return (guesses == labels).float().sum()\n",
        "\n",
        "def save_model(model, epoch):\n",
        "    savefile = \"{}-e{}-{}.pt\".format('pytorch-sentiment', epoch, int(time.time()))\n",
        "    tlog('Saving model {}'.format(savefile))\n",
        "    path = os.path.join('models', savefile)\n",
        "    # recommended way from https://pytorch.org/docs/stable/notes/serialization.html\n",
        "    torch.save(model.state_dict(), path)\n",
        "    return savefile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5x7Ri44llU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# global device handle\n",
        "if not torch.cuda.is_available():\n",
        "    device = torch.device('cpu')\n",
        "    print('*** GPU not available - running on CPU. ***')\n",
        "else:\n",
        "    device = torch.device('cuda')\n",
        "    print('GPU ready to go!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xTM35uul_D9",
        "colab_type": "text"
      },
      "source": [
        "## Setting Up the Training Data\n",
        "\n",
        "Setting up natural language datasets in PyTorch is a bit different than setting up numeric time series or image data. For this reason, the PyTorch domain library `torchtext` includes a number of utilities that help with handling data for NLP tasks.\n",
        "\n",
        "### `Field` Objects\n",
        "\n",
        "NLP datasets are often presented in a structured form, such as a CSV or TSV file, where individual fields can be identified. To ease consumption of this data, `torchtext` offers a `Field` class, and related subclasses, as well as a `TabularDataset` class. The `Field` classes let you define datatypes and transformations (such as tokenization) on a field, and the `TabularDataset` takes a list of fields together with information about the file and its format, to parse the file into an internal representation that we'll find convenient. (See lines 10-26 in the code cell below.)\n",
        "\n",
        "For this iteration of the exercise, we will skip the first two fields in the dataset, and create a single field for each phrase to be analyzed, and a field for the sentiment score associated with that field.\n",
        "\n",
        "### Vocabularies and Embedding\n",
        "\n",
        "The standard way of representing words in a natural language vocabulary is with **\"one-hot\" vectors** (known as *unit vectors* in other mathematical contexts). For a three-word vocabulary, this would look like:\n",
        "\n",
        "```\n",
        "word0 = torch.tensor([1, 0, 0])\n",
        "word1 = torch.tensor([0, 1, 0])\n",
        "word2 = torch.tensor([0, 0, 1])\n",
        "```\n",
        "\n",
        "Native speakers of most languages know 15,000 to 20,000 words; you can see how the above representation would consume untenable amounts of memory for large datasets. For this reason, PyTorch represents one-hot vectors by storing a single integer, which is the *index* of the single \"hot\" entry of the word vector.\n",
        "\n",
        "Even with this optimization, the one-hot representation doesn't capture interesting features of the vocabulary, such as the semantic closeness of word pairs like \"fantastic\" and \"amazing\". The vocabulary vector space can be reduced further by *embedding* the vocabulary in a lower-dimensional space, which has the effect of reducing the complexity of the space in which we're trying to find ad descend gradients while capturing some of these relationships within the vocabulary.\n",
        "\n",
        "You can create and train your own embedding using the `torch.nn.Embedding` layer, or you can use one of the pre-trained embeddings made available through `torchtext`. Below, we make use of the GloVe word embedding when we build our vocabulary for the `phrase` field in line 29."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBTlMfsM4bBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset constants\n",
        "VOCAB_SIZE = 15000 # max size of vocabulary\n",
        "VOCAB_VECTORS = \"glove.6B.100d\" # Stanford NLP GloVe (global vectors) for word rep\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    tlog('Preparing data...')\n",
        "    phrases_fieldspec = data.Field(include_lengths=True, tokenize='spacy')\n",
        "    labels_fieldspec = data.LabelField(dtype=torch.int64, sequential=False)\n",
        "\n",
        "    fields = [\n",
        "        ('SKIP_phrase_id', None),\n",
        "        ('SKIP_sentence_id', None),\n",
        "        ('phrases', phrases_fieldspec),\n",
        "        ('labels', labels_fieldspec)\n",
        "    ]\n",
        "\n",
        "\n",
        "    train_data = data.TabularDataset(\n",
        "        'train.tsv', # path to file\n",
        "        'TSV', # file format\n",
        "        fields,\n",
        "        skip_header = True # we have a header row\n",
        "    )\n",
        "    \n",
        "    train_data, eval_data = train_data.split()\n",
        "    phrases_fieldspec.build_vocab(train_data, max_size=VOCAB_SIZE, vectors=VOCAB_VECTORS)\n",
        "    labels_fieldspec.build_vocab(train_data)\n",
        "    vocab_size = len(phrases_fieldspec.vocab)\n",
        "    output_size = len(labels_fieldspec.vocab)\n",
        "    \n",
        "    train_iter, eval_iter = data.BucketIterator.splits(\n",
        "        (train_data, eval_data), \n",
        "        batch_size=BATCH_SIZE,\n",
        "        device=device, sort=False, shuffle=True)\n",
        "    \n",
        "    tlog('Data prepared')\n",
        "    return train_iter, eval_iter, vocab_size, output_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yMDM2EitxCx",
        "colab_type": "text"
      },
      "source": [
        "## The Model\n",
        "\n",
        "The model below gets to about 62% accuracy on this particular sentiment analysis problem. Below, we describe a few key features of the model.\n",
        "\n",
        "### Embedding\n",
        "\n",
        "The first layer of the model is the embedding layer. We described vocabulary embeddings above; here, we show how to take the weights of the pretrained embedding and use them within your own model.\n",
        "\n",
        "Note that we have set `requires_grad` on the embedding layer's weights to `False` (line 17 below). This freezes the embedding, so that it does not participate in the learning process. Leaving `requires_grad` as `True` for this layer's weights would mean radically increasing the number of learning parameters in the model.\n",
        "\n",
        "### Recurrent Neural Networks\n",
        "\n",
        "A *recurrent neural network* RNN may refer to one of two things: The first is a type of neural network layer that maintains a \"hidden\" state, separate from its learning weights, that lets it carry forward state when it deals with sequential data. The second is a class of neural network layers that includes the basic RNN and more complex constructs employing similar techniques, which include the *gated recurrent unit* (GRU) and *long short-term memory* (LSTM).\n",
        "\n",
        "For this exercise, we will employ an LSTM. It adds architectural complexity to the model, but has the advantage that it is much less susceptible to the problems of *vanishing gradients* and *exploding gradients* that are often seen when training RNNs on long sequences. We'll use two LSTM layers to start (lines 19-23). One important thing to note about the LSTM is that instead of a single hidden state vector, it maintains a separate *memory cell* with separate mechanisms for determining when to \"remember\" or \"forget\" certain computation paths.\n",
        "\n",
        "### Classification\n",
        "\n",
        "We'll model our possible integer outputs from 0 to 4 as five possible output classes, and use a linear layer at the end of the model to perform the final classification (line 25).\n",
        "\n",
        "### The Computation Graph\n",
        "\n",
        "In the `forward()` method, there are a couple of interesting features to how the model layers are composed.\n",
        "\n",
        "First, note that the `hidden` state is passed out of the model when the computation returns, and is *passed back in* by the caller on subsequent calls. For the case in which the input hidden state is `None`, the model initializes it correctly.\n",
        "\n",
        "Second, the `hidden` state consumed and returned by `self.lstm`  is actually a tuple containing the hidden state vector and the memory cell. When we pass the output of the LSTM to the classifier, we specify it as `hidden[0][-1]`. The `[0]` specifies the first element of the (hidden, cell) tuple, and the [-1] specifies the hidden state returned by the final LSTM layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFJ3dT4g40-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model constants\n",
        "EMBEDDING_SIZE = 100 # must match dimensions in vocab vectors above\n",
        "HIDDEN_SIZE = 100\n",
        "OUTPUT_SIZE = 5 # 0 to 4\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "\n",
        "\n",
        "class SentimentAnalyzer(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_size, pretrained_embedding, hidden_size, output_size):\n",
        "        super(SentimentAnalyzer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.embedding.weight.data.copy_(pretrained_embedding)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            hidden_size=hidden_size\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, phrases, hidden):\n",
        "        if hidden is None: # tuple w/ 2 for LSTM, make one for RNN or GRU\n",
        "            hidden = (torch.zeros(NUM_LAYERS, BATCH_SIZE, self.hidden_size, dtype=torch.float).to(device),\n",
        "                      torch.zeros(NUM_LAYERS, BATCH_SIZE, self.hidden_size, dtype=torch.float).to(device))\n",
        "        x = self.embedding(phrases)\n",
        "        x, hidden = self.lstm(x, hidden)\n",
        "        x = self.fc(hidden[0][-1]) # remove [0] for RNN or GRU, [-1] is for layers\n",
        "#        return x.squeeze(0), hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pjcUBWt029o",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "The following code cells define some functions to support our model training process, the structure of which should be familiar by now. Go ahead and run them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiUg5J_Q6m1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(vocab_size, output_size, vectors):\n",
        "    tlog('Creating model...')\n",
        "    sa = SentimentAnalyzer(vocab_size, EMBEDDING_SIZE, vectors, HIDDEN_SIZE, output_size)\n",
        "    tlog('The model has {} trainable parameters'.format(count_model_params(sa)))\n",
        "    tlog(sa)\n",
        "    return sa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaIJfIbk65HC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, loss_fn, optimizer): # one epoch\n",
        "    curr_loss = 0.\n",
        "    curr_correct = 0.\n",
        "    hidden = None\n",
        "    model.train() # makes sure that training-only fns, like dropout, are active\n",
        "    \n",
        "    for batch in iterator:\n",
        "        # get the data\n",
        "        phrases, lengths = batch.phrases\n",
        "        \n",
        "        if phrases.shape[1] == BATCH_SIZE:        \n",
        "            # predict and learn\n",
        "            optimizer.zero_grad()\n",
        "            guesses, hidden = model(phrases, hidden)\n",
        "            loss = loss_fn(guesses, batch.labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            hidden[0].detach_() # or we get double-backward errors\n",
        "            hidden[1].detach_() # or we get double-backward errors\n",
        "\n",
        "            # measure\n",
        "            curr_loss += loss.item()\n",
        "            curr_correct += count_correct(torch.argmax(guesses, 1), batch.labels)\n",
        "        \n",
        "    return curr_loss / len(iterator), curr_correct / (len(iterator) * BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wU0mXmR7CL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, loss_fn):\n",
        "    curr_loss = 0.\n",
        "    curr_correct = 0.\n",
        "    hidden = None\n",
        "    model.eval() # makes sure that training-only fns, like dropout, are inactive\n",
        "    \n",
        "    with torch.no_grad(): # not training\n",
        "        for batch in iterator:\n",
        "            # get the data\n",
        "            phrases, lengths = batch.phrases\n",
        "            \n",
        "            if phrases.shape[1] == BATCH_SIZE:        \n",
        "                # predict\n",
        "                guesses, hidden = model(phrases, hidden) # .squeeze(1)\n",
        "                loss = loss_fn(guesses, batch.labels)\n",
        "\n",
        "                # measure\n",
        "                curr_loss += loss.item()\n",
        "                curr_correct += count_correct(guesses, batch.labels)\n",
        "\n",
        "    \n",
        "    return curr_loss / len(iterator), curr_correct / (len(iterator) * BATCH_SIZE)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMySlJKl1Cmg",
        "colab_type": "text"
      },
      "source": [
        "Below, we define the training loop. The basic structures of bookkeeping, training, evaluation, reporting, and saving the model should be familiar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDmduE-x7Cuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training loop constants\n",
        "EPOCHS = 30\n",
        "LR = 1e-3\n",
        "\n",
        "\n",
        "def learn(model, train_iter, eval_iter):\n",
        "    eval_losses = []\n",
        "    eval_accs = []\n",
        "    best_eval_acc = 0\n",
        "    \n",
        "    model = model.to(device)\n",
        "\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    # loss_fn = loss_fn.to(device)\n",
        "\n",
        "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(params, lr=LR)\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        tlog('EPOCH {} of {}'.format(epoch + 1, EPOCHS))\n",
        "        \n",
        "        train_loss, train_acc = train(model, train_iter, loss_fn, optimizer)\n",
        "        tlog('  Training loss {}   acc {}'.format(train_loss, train_acc))\n",
        "        \n",
        "        eval_loss, eval_acc = evaluate(model, eval_iter, loss_fn)\n",
        "        tlog('  Validation loss {}   acc {}'.format(eval_loss, eval_acc))\n",
        "        eval_losses.append(eval_loss)\n",
        "        eval_accs.append(eval_acc)\n",
        "        if eval_acc > best_eval_acc:\n",
        "            tlog('  *** New accuracy peak, saving model')\n",
        "            best_eval_acc = eval_acc\n",
        "            saved_model_filename = save_model(model, epoch + 1)\n",
        "    \n",
        "    tlog('DONE')\n",
        "    tlog('Best accuracy: {}'.format(best_eval_acc))\n",
        "    return eval_losses, eval_accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzlnsv0n1egp",
        "colab_type": "text"
      },
      "source": [
        "Let's create our dataset and build the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nql6BMtcWQIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_iter, e_iter, vocab_size, output_size = get_data()\n",
        "embedding_vectors = t_iter.dataset.fields['phrases'].vocab.vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G54ySpm1i1g",
        "colab_type": "text"
      },
      "source": [
        "And finally, let's create the model and train it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZLEmqGx7G_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sa = get_model(vocab_size, output_size, embedding_vectors)\n",
        "losses, accs = learn(sa, t_iter, e_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9DEahUhC1hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(losses)):\n",
        "    print('{} {}'.format(losses[i], accs[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}