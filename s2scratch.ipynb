{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from s2cnn import SO3Convolution\n",
    "from s2cnn import S2Convolution\n",
    "from s2cnn import so3_integrate\n",
    "from s2cnn import so3_near_identity_grid\n",
    "from s2cnn import s2_near_identity_grid\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input0():\n",
    "    return torch.zeros(1,60,60)\n",
    "\n",
    "def inputrand():\n",
    "    return torch.rand(1,60,60)\n",
    "\n",
    "def visualize_tensor_layer(t): # expecting 2D tensor of any size\n",
    "    assert len(t.shape) == 2\n",
    "    for i in range(t.shape[0]):\n",
    "        for j in range(t.shape[1]):\n",
    "            print('{}'.format('1' if t[i][j].item() > 0. else '0'), end='')\n",
    "        print('')\n",
    "\n",
    "def add_light_fuzz(t, fuzz_factor=0.02): # expecting N * 60 * 60, add 2% fuzz\n",
    "    assert fuzz_factor >= 0.\n",
    "    assert fuzz_factor < 1.\n",
    "    out = t.clone()\n",
    "    for i in range(60):\n",
    "        for j in range(60):\n",
    "            if random.random() < fuzz_factor: # 5%ish\n",
    "                out[:,i,j] = 1.\n",
    "    return out\n",
    "        \n",
    "# could pass in all 0s, or add this feature to existing tensor\n",
    "# applies to all layers\n",
    "def make_training_feature1(t): # expecting N * 60 * 60\n",
    "    for i in range(5, 35):\n",
    "        for j in range(5, 35):\n",
    "            x = i - 20\n",
    "            y = j - 20\n",
    "            r2 = x * x + y * y \n",
    "            if r2 > 64 and r2 < 144:\n",
    "                t[:,i,j] = 1.\n",
    "    return t\n",
    "\n",
    "def make_training_feature2(t): # expecting N * 60 * 60\n",
    "    for i in range(30, 60):\n",
    "        for j in range(30, 60):\n",
    "            x = i - 30\n",
    "            y = 60 - j # 0,0 @ ctr btm\n",
    "            if abs(x - y) <= 2:\n",
    "                t[:,i,j] = 1.\n",
    "    return t\n",
    "\n",
    "def make_training_feature3(t): # expecting N * 60 * 60\n",
    "    for i in range(30, 60):\n",
    "        for j in range(0, 30):\n",
    "            if abs(45 - i) <= 2 or abs(15 - j) <= 2:\n",
    "                t[:,i,j] = 1.\n",
    "    return t\n",
    "\n",
    "def make_training_feature4(t):\n",
    "    for i in range(0, 30):\n",
    "        for j in range(30, 60):\n",
    "            x = i\n",
    "            y = 0 - j\n",
    "            if abs(x - y) <= 2 or ((30 - x) - y) <= 2:\n",
    "                t[:,i,j] = 1.\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_vert(t): # expects 2d tensor\n",
    "    out = t.clone()\n",
    "    assert len(out.shape) == 2\n",
    "    \n",
    "    w = out.shape[0]\n",
    "    h = out.shape[1]\n",
    "    buffer = torch.zeros(h)\n",
    "    for i in range(w // 2):\n",
    "        buffer = out[i, :].clone() # i think i have to clone else broadcasting turns weird?\n",
    "        out[i, :] = out[w - i - 1, :].clone()\n",
    "        out[w - i - 1, :] = buffer\n",
    "    return out\n",
    "\n",
    "def reflect_horiz(t):\n",
    "    return reflect_vert(t.transpose(0,1)).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis each of N kernels separately\n",
    "# each out kernel is a 20^3 tensor in default config\n",
    "# changed to 5 * 10^3 for this run\n",
    "def visualize_s2conv_out(t):\n",
    "    assert len(t.shape) == 4\n",
    "    for kernel in range(t.shape[0]):\n",
    "        print('KERNEL {}'.format(kernel))\n",
    "        outblock = []\n",
    "        for i in range(t.shape[1]): # row * column * depth = grid * depth = block\n",
    "            outgrid = []\n",
    "            for j in range(t.shape[2]): # row * column = grid\n",
    "                outrow = ''\n",
    "                for k in range(t.shape[3]): # row\n",
    "                    outrow += '1' if t[kernel,i,j,k] > 0. else '0'\n",
    "                outgrid.append(outrow)\n",
    "            outblock.append(outgrid)\n",
    "            \n",
    "        visblock = []\n",
    "        for i in range(len(outblock[0])):\n",
    "            visblock.append([])\n",
    "        for i in range(len(outblock)): # pull each grid from block\n",
    "            grid = outblock[i]\n",
    "            for j in range(len(grid)): # pull each row from grid\n",
    "                visblock[j].append(grid[j])\n",
    "        for i in range(len(visblock)):\n",
    "            print(' '.join(visblock[i]))\n",
    "                    \n",
    "def show_conv_layer_for(t, model): # expects 1 * 60 * 60 tensor\n",
    "    torch.unsqueeze(t, 0) # batch\n",
    "    _, _, t_conv = model(t)\n",
    "    visualize_s2conv_out(torch.squeeze(t_conv, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, n_inst=1000, real_only=False, fuzz_factor=None): # make 1000 instances of my training instances plus some bllshit\n",
    "        tf1 = make_training_feature1(input0()) # depends on global fns oops\n",
    "        tf2 = make_training_feature2(input0())\n",
    "        tf3 = make_training_feature3(input0())\n",
    "        tf4 = make_training_feature4(input0())\n",
    "        REAL_FEATURES = 4\n",
    "        MAX_FEATURES = 5\n",
    "        self.instances = []\n",
    "        for _ in range(n_inst):\n",
    "            r = random.randrange(REAL_FEATURES if real_only else MAX_FEATURES)\n",
    "            new_instance = None\n",
    "            if r == 0:\n",
    "                new_instance = tf1 # circle in upper left\n",
    "            elif r == 1:\n",
    "                new_instance = tf2 # SW-NE diag line in lower right\n",
    "            elif r == 2:\n",
    "                new_instance = tf3 # cross upper right\n",
    "            elif r == 3:\n",
    "                new_instance = tf4 # diag cross lower right\n",
    "            else:\n",
    "                new_instance = inputrand() # bullshit\n",
    "            \n",
    "            assert not (new_instance is None)\n",
    "            if (not (fuzz_factor is None)) and r < REAL_FEATURES:\n",
    "                new_instance = add_light_fuzz(new_instance, fuzz_factor=fuzz_factor)\n",
    "            self.instances.append((new_instance, r))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.instances[idx] # tensor, class idx\n",
    "\n",
    "training_dataset = DummyDataset()\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=4, shuffle=True)\n",
    "testing_dataset = DummyDataset(n_inst = 100, real_only=True, fuzz_factor=0.01)\n",
    "testing_loader = torch.utils.data.DataLoader(testing_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2TestModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(S2TestModel, self).__init__()\n",
    "        self.conv1 = S2Convolution(nfeature_in=1, nfeature_out=10, b_in=30, b_out=5, grid=s2_near_identity_grid())\n",
    "        self.fc1 = torch.nn.Linear(10, 20) # i am just fucking around with this architecture - will it learn?\n",
    "        self.fc2 = torch.nn.Linear(20, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        conv_out = x\n",
    "        # print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = so3_integrate(x)\n",
    "        int_out = x\n",
    "        # print(x.shape)\n",
    "        # activation?\n",
    "        x = self.fc1(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # print(x.shape)\n",
    "        return x, int_out, conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlog(s):\n",
    "    print('{}: {}'.format(time.asctime(), s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 4\n",
    "FUZZ_FACTOR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = DummyDataset(fuzz_factor=FUZZ_FACTOR)\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testing_dataset = DummyDataset(n_inst=100, real_only=True, fuzz_factor=FUZZ_FACTOR)\n",
    "testing_loader = torch.utils.data.DataLoader(testing_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model = S2TestModel()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 17:59:34 2019: EPOCH 1 of 100\n",
      "load 2.pkl.gz... done\n",
      "load 2.pkl.gz... done\n",
      "load 4.pkl.gz... done\n",
      "Thu Apr 18 17:59:34 2019: iter 50/250  loss 1.7012929916381836  running loss 72.44314575195312\n",
      "Thu Apr 18 17:59:35 2019: iter 100/250  loss 1.2336326837539673  running loss 138.89939546585083\n",
      "Thu Apr 18 17:59:35 2019: iter 150/250  loss 1.244809627532959  running loss 201.81333076953888\n",
      "Thu Apr 18 17:59:36 2019: iter 200/250  loss 1.3569700717926025  running loss 263.418460637331\n",
      "Thu Apr 18 17:59:36 2019: iter 250/250  loss 0.780707597732544  running loss 319.1817424297333\n",
      "Thu Apr 18 17:59:36 2019: Test accuracy for epoch 0: 0.31\n",
      "Thu Apr 18 17:59:36 2019: EPOCH 2 of 100\n",
      "Thu Apr 18 17:59:37 2019: iter 50/250  loss 0.6877192854881287  running loss 54.09107583761215\n",
      "Thu Apr 18 17:59:37 2019: iter 100/250  loss 0.6392704248428345  running loss 103.9368489086628\n",
      "Thu Apr 18 17:59:38 2019: iter 150/250  loss 0.6217330098152161  running loss 155.21555760502815\n",
      "Thu Apr 18 17:59:38 2019: iter 200/250  loss 0.9097512364387512  running loss 204.05981793999672\n",
      "Thu Apr 18 17:59:39 2019: iter 250/250  loss 0.7804495096206665  running loss 246.84927156567574\n",
      "Thu Apr 18 17:59:39 2019: Test accuracy for epoch 1: 0.81\n",
      "Thu Apr 18 17:59:39 2019: EPOCH 3 of 100\n",
      "Thu Apr 18 17:59:40 2019: iter 50/250  loss 1.004324197769165  running loss 40.74620282649994\n",
      "Thu Apr 18 17:59:40 2019: iter 100/250  loss 0.6758806109428406  running loss 80.21988198161125\n",
      "Thu Apr 18 17:59:40 2019: iter 150/250  loss 0.8524775505065918  running loss 116.48791781067848\n",
      "Thu Apr 18 17:59:41 2019: iter 200/250  loss 0.9002159833908081  running loss 153.45310309529305\n",
      "Thu Apr 18 17:59:41 2019: iter 250/250  loss 0.8165936470031738  running loss 187.32216829061508\n",
      "Thu Apr 18 17:59:42 2019: Test accuracy for epoch 2: 0.78\n",
      "Thu Apr 18 17:59:42 2019: EPOCH 4 of 100\n",
      "Thu Apr 18 17:59:42 2019: iter 50/250  loss 0.6235321760177612  running loss 31.745471596717834\n",
      "Thu Apr 18 17:59:43 2019: iter 100/250  loss 0.4631331264972687  running loss 58.51245856285095\n",
      "Thu Apr 18 17:59:43 2019: iter 150/250  loss 0.4645235538482666  running loss 86.73258751630783\n",
      "Thu Apr 18 17:59:44 2019: iter 200/250  loss 0.5431600213050842  running loss 112.94945329427719\n",
      "Thu Apr 18 17:59:44 2019: iter 250/250  loss 0.5247765183448792  running loss 138.77437868714333\n",
      "Thu Apr 18 17:59:45 2019: Test accuracy for epoch 3: 0.78\n",
      "Thu Apr 18 17:59:45 2019: EPOCH 5 of 100\n",
      "Thu Apr 18 17:59:45 2019: iter 50/250  loss 0.402924120426178  running loss 23.480876326560974\n",
      "Thu Apr 18 17:59:46 2019: iter 100/250  loss 0.4087066650390625  running loss 45.81267240643501\n",
      "Thu Apr 18 17:59:46 2019: iter 150/250  loss 0.39792943000793457  running loss 66.14423862099648\n",
      "Thu Apr 18 17:59:47 2019: iter 200/250  loss 0.4813811182975769  running loss 85.34623286128044\n",
      "Thu Apr 18 17:59:48 2019: iter 250/250  loss 0.5703637599945068  running loss 105.71387842297554\n",
      "Thu Apr 18 17:59:48 2019: Test accuracy for epoch 4: 0.89\n",
      "Thu Apr 18 17:59:48 2019: EPOCH 6 of 100\n",
      "Thu Apr 18 17:59:49 2019: iter 50/250  loss 0.07979381084442139  running loss 17.950752437114716\n",
      "Thu Apr 18 17:59:49 2019: iter 100/250  loss 0.2707746624946594  running loss 36.865637958049774\n",
      "Thu Apr 18 17:59:50 2019: iter 150/250  loss 0.18696504831314087  running loss 53.570271611213684\n",
      "Thu Apr 18 17:59:50 2019: iter 200/250  loss 0.5584282875061035  running loss 71.30779939889908\n",
      "Thu Apr 18 17:59:51 2019: iter 250/250  loss 0.46902287006378174  running loss 86.5427918434143\n",
      "Thu Apr 18 17:59:51 2019: Test accuracy for epoch 5: 0.81\n",
      "Thu Apr 18 17:59:51 2019: EPOCH 7 of 100\n",
      "Thu Apr 18 17:59:51 2019: iter 50/250  loss 0.3763553500175476  running loss 15.706025063991547\n",
      "Thu Apr 18 17:59:52 2019: iter 100/250  loss 0.5259847044944763  running loss 30.32187968492508\n",
      "Thu Apr 18 17:59:52 2019: iter 150/250  loss 0.5350574254989624  running loss 46.67914694547653\n",
      "Thu Apr 18 17:59:53 2019: iter 200/250  loss 0.09270858764648438  running loss 61.06906110048294\n",
      "Thu Apr 18 17:59:53 2019: iter 250/250  loss 0.1615174412727356  running loss 76.00869899988174\n",
      "Thu Apr 18 17:59:54 2019: Test accuracy for epoch 6: 0.78\n",
      "Thu Apr 18 17:59:54 2019: EPOCH 8 of 100\n",
      "Thu Apr 18 17:59:54 2019: iter 50/250  loss 0.32703810930252075  running loss 13.99132615327835\n",
      "Thu Apr 18 17:59:55 2019: iter 100/250  loss 0.5319353342056274  running loss 26.640170633792877\n",
      "Thu Apr 18 17:59:55 2019: iter 150/250  loss 0.27852046489715576  running loss 40.841075360774994\n",
      "Thu Apr 18 17:59:56 2019: iter 200/250  loss 0.1899430751800537  running loss 54.41063106060028\n",
      "Thu Apr 18 17:59:57 2019: iter 250/250  loss 0.40001577138900757  running loss 67.42412549257278\n",
      "Thu Apr 18 17:59:57 2019: Test accuracy for epoch 7: 1.0\n",
      "Thu Apr 18 17:59:57 2019: EPOCH 9 of 100\n",
      "Thu Apr 18 17:59:57 2019: iter 50/250  loss 0.3021122217178345  running loss 14.247425973415375\n",
      "Thu Apr 18 17:59:58 2019: iter 100/250  loss 0.13831186294555664  running loss 26.31407457590103\n",
      "Thu Apr 18 17:59:58 2019: iter 150/250  loss 0.13260018825531006  running loss 38.04377120733261\n",
      "Thu Apr 18 17:59:59 2019: iter 200/250  loss 0.23790991306304932  running loss 49.02973395586014\n",
      "Thu Apr 18 18:00:00 2019: iter 250/250  loss 0.3030397891998291  running loss 60.496121287345886\n",
      "Thu Apr 18 18:00:00 2019: Test accuracy for epoch 8: 1.0\n",
      "Thu Apr 18 18:00:00 2019: EPOCH 10 of 100\n",
      "Thu Apr 18 18:00:01 2019: iter 50/250  loss 0.037058234214782715  running loss 11.507328450679779\n",
      "Thu Apr 18 18:00:01 2019: iter 100/250  loss 0.35968416929244995  running loss 22.8975111246109\n",
      "Thu Apr 18 18:00:02 2019: iter 150/250  loss 0.3737208843231201  running loss 34.03896564245224\n",
      "Thu Apr 18 18:00:02 2019: iter 200/250  loss 0.11718106269836426  running loss 44.41112744808197\n",
      "Thu Apr 18 18:00:03 2019: iter 250/250  loss 0.3110830783843994  running loss 54.34971344470978\n",
      "Thu Apr 18 18:00:03 2019: Test accuracy for epoch 9: 0.94\n",
      "Thu Apr 18 18:00:03 2019: EPOCH 11 of 100\n",
      "Thu Apr 18 18:00:03 2019: iter 50/250  loss 0.28214359283447266  running loss 11.379205107688904\n",
      "Thu Apr 18 18:00:04 2019: iter 100/250  loss 0.008408069610595703  running loss 19.080694437026978\n",
      "Thu Apr 18 18:00:05 2019: iter 150/250  loss 0.4186372756958008  running loss 28.156404972076416\n",
      "Thu Apr 18 18:00:05 2019: iter 200/250  loss 0.1186298131942749  running loss 38.44135248661041\n",
      "Thu Apr 18 18:00:06 2019: iter 250/250  loss 0.09203827381134033  running loss 46.91299796104431\n",
      "Thu Apr 18 18:00:06 2019: Test accuracy for epoch 10: 0.99\n",
      "Thu Apr 18 18:00:06 2019: EPOCH 12 of 100\n",
      "Thu Apr 18 18:00:07 2019: iter 50/250  loss 0.3192552328109741  running loss 8.04187786579132\n",
      "Thu Apr 18 18:00:08 2019: iter 100/250  loss 0.10636007785797119  running loss 15.480597257614136\n",
      "Thu Apr 18 18:00:08 2019: iter 150/250  loss 0.008658170700073242  running loss 22.428638339042664\n",
      "Thu Apr 18 18:00:09 2019: iter 200/250  loss 0.16708457469940186  running loss 31.552087545394897\n",
      "Thu Apr 18 18:00:09 2019: iter 250/250  loss 0.32439982891082764  running loss 39.72156298160553\n",
      "Thu Apr 18 18:00:10 2019: Test accuracy for epoch 11: 1.0\n",
      "Thu Apr 18 18:00:10 2019: EPOCH 13 of 100\n",
      "Thu Apr 18 18:00:10 2019: iter 50/250  loss 0.3552839756011963  running loss 7.272369980812073\n",
      "Thu Apr 18 18:00:11 2019: iter 100/250  loss 0.07623875141143799  running loss 12.932502508163452\n",
      "Thu Apr 18 18:00:11 2019: iter 150/250  loss 0.05281257629394531  running loss 19.298701524734497\n",
      "Thu Apr 18 18:00:12 2019: iter 200/250  loss 0.1434229612350464  running loss 26.38051176071167\n",
      "Thu Apr 18 18:00:13 2019: iter 250/250  loss 0.32311713695526123  running loss 32.63068246841431\n",
      "Thu Apr 18 18:00:13 2019: Test accuracy for epoch 12: 0.98\n",
      "Thu Apr 18 18:00:13 2019: EPOCH 14 of 100\n",
      "Thu Apr 18 18:00:14 2019: iter 50/250  loss 0.025133728981018066  running loss 6.176059365272522\n",
      "Thu Apr 18 18:00:14 2019: iter 100/250  loss 0.11684715747833252  running loss 12.00858986377716\n",
      "Thu Apr 18 18:00:15 2019: iter 150/250  loss 0.04427289962768555  running loss 16.548574328422546\n",
      "Thu Apr 18 18:00:15 2019: iter 200/250  loss 0.0599365234375  running loss 22.87746822834015\n",
      "Thu Apr 18 18:00:16 2019: iter 250/250  loss 0.08940303325653076  running loss 27.10378670692444\n",
      "Thu Apr 18 18:00:16 2019: Test accuracy for epoch 13: 1.0\n",
      "Thu Apr 18 18:00:16 2019: EPOCH 15 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 18:00:17 2019: iter 50/250  loss 0.04784655570983887  running loss 4.142983675003052\n",
      "Thu Apr 18 18:00:18 2019: iter 100/250  loss 0.06721460819244385  running loss 8.751645803451538\n",
      "Thu Apr 18 18:00:18 2019: iter 150/250  loss 0.4835399389266968  running loss 12.939526557922363\n",
      "Thu Apr 18 18:00:19 2019: iter 200/250  loss 0.07257747650146484  running loss 17.086385369300842\n",
      "Thu Apr 18 18:00:19 2019: iter 250/250  loss 0.04731476306915283  running loss 20.94782519340515\n",
      "Thu Apr 18 18:00:20 2019: Test accuracy for epoch 14: 1.0\n",
      "Thu Apr 18 18:00:20 2019: EPOCH 16 of 100\n",
      "Thu Apr 18 18:00:20 2019: iter 50/250  loss 0.06874048709869385  running loss 4.195220112800598\n",
      "Thu Apr 18 18:00:21 2019: iter 100/250  loss 0.14701485633850098  running loss 8.043732523918152\n",
      "Thu Apr 18 18:00:21 2019: iter 150/250  loss 0.011994123458862305  running loss 11.223838329315186\n",
      "Thu Apr 18 18:00:22 2019: iter 200/250  loss 0.13698136806488037  running loss 14.00735354423523\n",
      "Thu Apr 18 18:00:22 2019: iter 250/250  loss 0.05627334117889404  running loss 16.859492778778076\n",
      "Thu Apr 18 18:00:23 2019: Test accuracy for epoch 15: 1.0\n",
      "Thu Apr 18 18:00:23 2019: EPOCH 17 of 100\n",
      "Thu Apr 18 18:00:23 2019: iter 50/250  loss 0.05548703670501709  running loss 2.6561437845230103\n",
      "Thu Apr 18 18:00:24 2019: iter 100/250  loss 0.0573275089263916  running loss 5.561067342758179\n",
      "Thu Apr 18 18:00:24 2019: iter 150/250  loss 0.026180505752563477  running loss 8.270218014717102\n",
      "Thu Apr 18 18:00:25 2019: iter 200/250  loss 0.01099538803100586  running loss 11.17146921157837\n",
      "Thu Apr 18 18:00:26 2019: iter 250/250  loss 0.03846085071563721  running loss 13.774928331375122\n",
      "Thu Apr 18 18:00:26 2019: Test accuracy for epoch 16: 1.0\n",
      "Thu Apr 18 18:00:26 2019: EPOCH 18 of 100\n",
      "Thu Apr 18 18:00:26 2019: iter 50/250  loss 0.0060651302337646484  running loss 2.033391833305359\n",
      "Thu Apr 18 18:00:27 2019: iter 100/250  loss 0.04754996299743652  running loss 4.1961153745651245\n",
      "Thu Apr 18 18:00:28 2019: iter 150/250  loss 0.10939514636993408  running loss 6.613165497779846\n",
      "Thu Apr 18 18:00:28 2019: iter 200/250  loss 0.006223440170288086  running loss 8.698158502578735\n",
      "Thu Apr 18 18:00:29 2019: iter 250/250  loss 0.03081834316253662  running loss 11.039002895355225\n",
      "Thu Apr 18 18:00:29 2019: Test accuracy for epoch 17: 1.0\n",
      "Thu Apr 18 18:00:29 2019: EPOCH 19 of 100\n",
      "Thu Apr 18 18:00:30 2019: iter 50/250  loss 0.03227221965789795  running loss 2.1441179513931274\n",
      "Thu Apr 18 18:00:30 2019: iter 100/250  loss 0.012012004852294922  running loss 3.81298291683197\n",
      "Thu Apr 18 18:00:31 2019: iter 150/250  loss 0.048688292503356934  running loss 5.636152744293213\n",
      "Thu Apr 18 18:00:31 2019: iter 200/250  loss 0.01950216293334961  running loss 7.525360822677612\n",
      "Thu Apr 18 18:00:32 2019: iter 250/250  loss 0.03749418258666992  running loss 9.187885880470276\n",
      "Thu Apr 18 18:00:32 2019: Test accuracy for epoch 18: 1.0\n",
      "Thu Apr 18 18:00:32 2019: EPOCH 20 of 100\n",
      "Thu Apr 18 18:00:33 2019: iter 50/250  loss 0.04612016677856445  running loss 1.6895781755447388\n",
      "Thu Apr 18 18:00:33 2019: iter 100/250  loss 0.018001317977905273  running loss 3.371480703353882\n",
      "Thu Apr 18 18:00:34 2019: iter 150/250  loss 0.0011744499206542969  running loss 4.7783437967300415\n",
      "Thu Apr 18 18:00:34 2019: iter 200/250  loss 0.03190875053405762  running loss 6.375675439834595\n",
      "Thu Apr 18 18:00:35 2019: iter 250/250  loss 0.014568090438842773  running loss 8.085302710533142\n",
      "Thu Apr 18 18:00:35 2019: Test accuracy for epoch 19: 1.0\n",
      "Thu Apr 18 18:00:35 2019: EPOCH 21 of 100\n",
      "Thu Apr 18 18:00:36 2019: iter 50/250  loss 0.03447234630584717  running loss 1.3409833908081055\n",
      "Thu Apr 18 18:00:36 2019: iter 100/250  loss 0.022889018058776855  running loss 2.7516831159591675\n",
      "Thu Apr 18 18:00:37 2019: iter 150/250  loss 0.019289255142211914  running loss 3.894924521446228\n",
      "Thu Apr 18 18:00:38 2019: iter 200/250  loss 0.01248621940612793  running loss 5.145948171615601\n",
      "Thu Apr 18 18:00:38 2019: iter 250/250  loss 0.04407238960266113  running loss 6.80574893951416\n",
      "Thu Apr 18 18:00:38 2019: Test accuracy for epoch 20: 1.0\n",
      "Thu Apr 18 18:00:38 2019: EPOCH 22 of 100\n",
      "Thu Apr 18 18:00:39 2019: iter 50/250  loss 0.020158767700195312  running loss 1.3367176055908203\n",
      "Thu Apr 18 18:00:40 2019: iter 100/250  loss 0.01583075523376465  running loss 2.546333074569702\n",
      "Thu Apr 18 18:00:40 2019: iter 150/250  loss 0.01930701732635498  running loss 3.8748176097869873\n",
      "Thu Apr 18 18:00:41 2019: iter 200/250  loss 0.019983768463134766  running loss 4.914618134498596\n",
      "Thu Apr 18 18:00:41 2019: iter 250/250  loss 0.013305902481079102  running loss 5.957961201667786\n",
      "Thu Apr 18 18:00:42 2019: Test accuracy for epoch 21: 1.0\n",
      "Thu Apr 18 18:00:42 2019: EPOCH 23 of 100\n",
      "Thu Apr 18 18:00:42 2019: iter 50/250  loss 0.011387348175048828  running loss 1.1074458360671997\n",
      "Thu Apr 18 18:00:43 2019: iter 100/250  loss 0.026732683181762695  running loss 2.085336208343506\n",
      "Thu Apr 18 18:00:43 2019: iter 150/250  loss 0.009313583374023438  running loss 3.3323378562927246\n",
      "Thu Apr 18 18:00:44 2019: iter 200/250  loss 0.01792442798614502  running loss 4.39391028881073\n",
      "Thu Apr 18 18:00:45 2019: iter 250/250  loss 0.0038900375366210938  running loss 5.258443355560303\n",
      "Thu Apr 18 18:00:45 2019: Test accuracy for epoch 22: 1.0\n",
      "Thu Apr 18 18:00:45 2019: EPOCH 24 of 100\n",
      "Thu Apr 18 18:00:46 2019: iter 50/250  loss 0.0034160614013671875  running loss 0.8963598012924194\n",
      "Thu Apr 18 18:00:46 2019: iter 100/250  loss 0.02978229522705078  running loss 1.9453262090682983\n",
      "Thu Apr 18 18:00:47 2019: iter 150/250  loss 0.010082840919494629  running loss 2.801535129547119\n",
      "Thu Apr 18 18:00:47 2019: iter 200/250  loss 0.00502324104309082  running loss 3.571545362472534\n",
      "Thu Apr 18 18:00:48 2019: iter 250/250  loss 0.012425899505615234  running loss 4.391708493232727\n",
      "Thu Apr 18 18:00:48 2019: Test accuracy for epoch 23: 1.0\n",
      "Thu Apr 18 18:00:48 2019: EPOCH 25 of 100\n",
      "Thu Apr 18 18:00:49 2019: iter 50/250  loss 0.010795354843139648  running loss 0.7773842811584473\n",
      "Thu Apr 18 18:00:49 2019: iter 100/250  loss 0.015405535697937012  running loss 1.5444374084472656\n",
      "Thu Apr 18 18:00:50 2019: iter 150/250  loss 0.007220029830932617  running loss 2.3378535509109497\n",
      "Thu Apr 18 18:00:50 2019: iter 200/250  loss 0.006827831268310547  running loss 3.3689301013946533\n",
      "Thu Apr 18 18:00:51 2019: iter 250/250  loss 0.012606501579284668  running loss 4.207093238830566\n",
      "Thu Apr 18 18:00:51 2019: Test accuracy for epoch 24: 1.0\n",
      "Thu Apr 18 18:00:51 2019: EPOCH 26 of 100\n",
      "Thu Apr 18 18:00:52 2019: iter 50/250  loss 0.0042421817779541016  running loss 0.6404746770858765\n",
      "Thu Apr 18 18:00:52 2019: iter 100/250  loss 0.0023555755615234375  running loss 1.7047978639602661\n",
      "Thu Apr 18 18:00:53 2019: iter 150/250  loss 0.007834792137145996  running loss 2.4006482362747192\n",
      "Thu Apr 18 18:00:53 2019: iter 200/250  loss 0.014394760131835938  running loss 3.0798044204711914\n",
      "Thu Apr 18 18:00:54 2019: iter 250/250  loss 0.004387617111206055  running loss 3.803831100463867\n",
      "Thu Apr 18 18:00:54 2019: Test accuracy for epoch 25: 1.0\n",
      "Thu Apr 18 18:00:54 2019: EPOCH 27 of 100\n",
      "Thu Apr 18 18:00:55 2019: iter 50/250  loss 0.005744218826293945  running loss 0.7331713438034058\n",
      "Thu Apr 18 18:00:55 2019: iter 100/250  loss 0.00733494758605957  running loss 1.585079312324524\n",
      "Thu Apr 18 18:00:56 2019: iter 150/250  loss 0.007448554039001465  running loss 2.1847840547561646\n",
      "Thu Apr 18 18:00:56 2019: iter 200/250  loss 0.011251091957092285  running loss 2.721611499786377\n",
      "Thu Apr 18 18:00:57 2019: iter 250/250  loss 0.03165745735168457  running loss 3.369481325149536\n",
      "Thu Apr 18 18:00:57 2019: Test accuracy for epoch 26: 1.0\n",
      "Thu Apr 18 18:00:57 2019: EPOCH 28 of 100\n",
      "Thu Apr 18 18:00:58 2019: iter 50/250  loss 0.004023075103759766  running loss 0.6095510721206665\n",
      "Thu Apr 18 18:00:58 2019: iter 100/250  loss 0.026751399040222168  running loss 1.4550920724868774\n",
      "Thu Apr 18 18:00:59 2019: iter 150/250  loss 0.0032129287719726562  running loss 2.0478968620300293\n",
      "Thu Apr 18 18:01:00 2019: iter 200/250  loss 0.002606630325317383  running loss 2.5450291633605957\n",
      "Thu Apr 18 18:01:00 2019: iter 250/250  loss 0.012737512588500977  running loss 3.1178900003433228\n",
      "Thu Apr 18 18:01:00 2019: Test accuracy for epoch 27: 1.0\n",
      "Thu Apr 18 18:01:00 2019: EPOCH 29 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 18:01:01 2019: iter 50/250  loss 0.013907551765441895  running loss 0.5769209861755371\n",
      "Thu Apr 18 18:01:02 2019: iter 100/250  loss 0.010252237319946289  running loss 1.1634210348129272\n",
      "Thu Apr 18 18:01:02 2019: iter 150/250  loss 0.006017804145812988  running loss 1.634366512298584\n",
      "Thu Apr 18 18:01:03 2019: iter 200/250  loss 0.009217143058776855  running loss 2.263617515563965\n",
      "Thu Apr 18 18:01:03 2019: iter 250/250  loss 0.02204275131225586  running loss 2.7139729261398315\n",
      "Thu Apr 18 18:01:04 2019: Test accuracy for epoch 28: 1.0\n",
      "Thu Apr 18 18:01:04 2019: EPOCH 30 of 100\n",
      "Thu Apr 18 18:01:04 2019: iter 50/250  loss 0.002591252326965332  running loss 0.5044167041778564\n",
      "Thu Apr 18 18:01:05 2019: iter 100/250  loss 0.019701480865478516  running loss 1.3006447553634644\n",
      "Thu Apr 18 18:01:05 2019: iter 150/250  loss 0.004873394966125488  running loss 1.7449367046356201\n",
      "Thu Apr 18 18:01:06 2019: iter 200/250  loss 0.005588054656982422  running loss 2.292712450027466\n",
      "Thu Apr 18 18:01:06 2019: iter 250/250  loss 0.00654911994934082  running loss 2.7494561672210693\n",
      "Thu Apr 18 18:01:07 2019: Test accuracy for epoch 29: 1.0\n",
      "Thu Apr 18 18:01:07 2019: EPOCH 31 of 100\n",
      "Thu Apr 18 18:01:07 2019: iter 50/250  loss 0.01323699951171875  running loss 0.47220754623413086\n",
      "Thu Apr 18 18:01:08 2019: iter 100/250  loss 0.0043392181396484375  running loss 0.9498547315597534\n",
      "Thu Apr 18 18:01:08 2019: iter 150/250  loss 0.003943920135498047  running loss 1.4388617277145386\n",
      "Thu Apr 18 18:01:09 2019: iter 200/250  loss 0.0026586055755615234  running loss 2.1274462938308716\n",
      "Thu Apr 18 18:01:09 2019: iter 250/250  loss 0.010227680206298828  running loss 2.5014554262161255\n",
      "Thu Apr 18 18:01:10 2019: Test accuracy for epoch 30: 1.0\n",
      "Thu Apr 18 18:01:10 2019: EPOCH 32 of 100\n",
      "Thu Apr 18 18:01:10 2019: iter 50/250  loss 0.0005054473876953125  running loss 0.4203016757965088\n",
      "Thu Apr 18 18:01:11 2019: iter 100/250  loss 0.009508728981018066  running loss 0.7982268333435059\n",
      "Thu Apr 18 18:01:11 2019: iter 150/250  loss 0.004014492034912109  running loss 1.4033796787261963\n",
      "Thu Apr 18 18:01:12 2019: iter 200/250  loss 0.016131162643432617  running loss 1.8664791584014893\n",
      "Thu Apr 18 18:01:13 2019: iter 250/250  loss 0.0018610954284667969  running loss 2.2665975093841553\n",
      "Thu Apr 18 18:01:13 2019: Test accuracy for epoch 31: 1.0\n",
      "Thu Apr 18 18:01:13 2019: EPOCH 33 of 100\n",
      "Thu Apr 18 18:01:13 2019: iter 50/250  loss 0.012830376625061035  running loss 0.39791083335876465\n",
      "Thu Apr 18 18:01:14 2019: iter 100/250  loss 0.005199909210205078  running loss 0.7513096332550049\n",
      "Thu Apr 18 18:01:15 2019: iter 150/250  loss 0.007173061370849609  running loss 1.0758765935897827\n",
      "Thu Apr 18 18:01:15 2019: iter 200/250  loss 0.0015511512756347656  running loss 1.4720278978347778\n",
      "Thu Apr 18 18:01:16 2019: iter 250/250  loss 0.006538510322570801  running loss 2.203299641609192\n",
      "Thu Apr 18 18:01:16 2019: Test accuracy for epoch 32: 1.0\n",
      "Thu Apr 18 18:01:16 2019: EPOCH 34 of 100\n",
      "Thu Apr 18 18:01:17 2019: iter 50/250  loss 0.0018935203552246094  running loss 0.5090532302856445\n",
      "Thu Apr 18 18:01:17 2019: iter 100/250  loss 0.006037592887878418  running loss 0.8607349395751953\n",
      "Thu Apr 18 18:01:18 2019: iter 150/250  loss 0.00419461727142334  running loss 1.188905119895935\n",
      "Thu Apr 18 18:01:18 2019: iter 200/250  loss 0.008477210998535156  running loss 1.5601294040679932\n",
      "Thu Apr 18 18:01:19 2019: iter 250/250  loss 0.0025413036346435547  running loss 1.9386532306671143\n",
      "Thu Apr 18 18:01:19 2019: Test accuracy for epoch 33: 1.0\n",
      "Thu Apr 18 18:01:19 2019: EPOCH 35 of 100\n",
      "Thu Apr 18 18:01:20 2019: iter 50/250  loss 0.005627751350402832  running loss 0.3547196388244629\n",
      "Thu Apr 18 18:01:21 2019: iter 100/250  loss 0.009755969047546387  running loss 0.6665263175964355\n",
      "Thu Apr 18 18:01:21 2019: iter 150/250  loss 0.0019016265869140625  running loss 0.9279266595840454\n",
      "Thu Apr 18 18:01:22 2019: iter 200/250  loss 0.010922431945800781  running loss 1.4747562408447266\n",
      "Thu Apr 18 18:01:23 2019: iter 250/250  loss 0.002071857452392578  running loss 1.8546278476715088\n",
      "Thu Apr 18 18:01:23 2019: Test accuracy for epoch 34: 1.0\n",
      "Thu Apr 18 18:01:23 2019: EPOCH 36 of 100\n",
      "Thu Apr 18 18:01:23 2019: iter 50/250  loss 0.0032835006713867188  running loss 0.6776375770568848\n",
      "Thu Apr 18 18:01:24 2019: iter 100/250  loss 0.023946642875671387  running loss 1.0066308975219727\n",
      "Thu Apr 18 18:01:25 2019: iter 150/250  loss 0.0013751983642578125  running loss 1.3190120458602905\n",
      "Thu Apr 18 18:01:25 2019: iter 200/250  loss 0.00991201400756836  running loss 1.621511697769165\n",
      "Thu Apr 18 18:01:26 2019: iter 250/250  loss 0.014315128326416016  running loss 1.952907919883728\n",
      "Thu Apr 18 18:01:26 2019: Test accuracy for epoch 35: 1.0\n",
      "Thu Apr 18 18:01:26 2019: EPOCH 37 of 100\n",
      "Thu Apr 18 18:01:27 2019: iter 50/250  loss 0.0013151168823242188  running loss 0.5334930419921875\n",
      "Thu Apr 18 18:01:27 2019: iter 100/250  loss 0.007607698440551758  running loss 0.8102995157241821\n",
      "Thu Apr 18 18:01:28 2019: iter 150/250  loss 0.01656317710876465  running loss 1.166014552116394\n",
      "Thu Apr 18 18:01:29 2019: iter 200/250  loss 0.0052460432052612305  running loss 1.432357907295227\n",
      "Thu Apr 18 18:01:29 2019: iter 250/250  loss 0.00400853157043457  running loss 1.6983569860458374\n",
      "Thu Apr 18 18:01:29 2019: Test accuracy for epoch 36: 1.0\n",
      "Thu Apr 18 18:01:29 2019: EPOCH 38 of 100\n",
      "Thu Apr 18 18:01:30 2019: iter 50/250  loss 0.0017061233520507812  running loss 0.2482433319091797\n",
      "Thu Apr 18 18:01:31 2019: iter 100/250  loss 0.0018091201782226562  running loss 0.4975402355194092\n",
      "Thu Apr 18 18:01:31 2019: iter 150/250  loss 0.004252791404724121  running loss 1.1804890632629395\n",
      "Thu Apr 18 18:01:32 2019: iter 200/250  loss 0.0008807182312011719  running loss 1.4738134145736694\n",
      "Thu Apr 18 18:01:32 2019: iter 250/250  loss 0.0032362937927246094  running loss 1.7506043910980225\n",
      "Thu Apr 18 18:01:32 2019: Test accuracy for epoch 37: 1.0\n",
      "Thu Apr 18 18:01:32 2019: EPOCH 39 of 100\n",
      "Thu Apr 18 18:01:33 2019: iter 50/250  loss 0.010608077049255371  running loss 0.3013730049133301\n",
      "Thu Apr 18 18:01:34 2019: iter 100/250  loss 0.003999471664428711  running loss 0.5680670738220215\n",
      "Thu Apr 18 18:01:34 2019: iter 150/250  loss 0.0005288124084472656  running loss 0.8392608165740967\n",
      "Thu Apr 18 18:01:35 2019: iter 200/250  loss 0.008016109466552734  running loss 1.1018097400665283\n",
      "Thu Apr 18 18:01:35 2019: iter 250/250  loss 0.0013155937194824219  running loss 1.4641532897949219\n",
      "Thu Apr 18 18:01:35 2019: Test accuracy for epoch 38: 1.0\n",
      "Thu Apr 18 18:01:35 2019: EPOCH 40 of 100\n",
      "Thu Apr 18 18:01:36 2019: iter 50/250  loss 0.0024297237396240234  running loss 0.3244757652282715\n",
      "Thu Apr 18 18:01:37 2019: iter 100/250  loss 0.0024042129516601562  running loss 0.6707320213317871\n",
      "Thu Apr 18 18:01:37 2019: iter 150/250  loss 0.010350704193115234  running loss 0.9100459814071655\n",
      "Thu Apr 18 18:01:38 2019: iter 200/250  loss 0.009373664855957031  running loss 1.4499146938323975\n",
      "Thu Apr 18 18:01:38 2019: iter 250/250  loss 0.009111285209655762  running loss 1.656048059463501\n",
      "Thu Apr 18 18:01:39 2019: Test accuracy for epoch 39: 1.0\n",
      "Thu Apr 18 18:01:39 2019: EPOCH 41 of 100\n",
      "Thu Apr 18 18:01:39 2019: iter 50/250  loss 0.0025420188903808594  running loss 0.292826771736145\n",
      "Thu Apr 18 18:01:40 2019: iter 100/250  loss 0.001241922378540039  running loss 0.5342384576797485\n",
      "Thu Apr 18 18:01:40 2019: iter 150/250  loss 0.00997161865234375  running loss 0.7632882595062256\n",
      "Thu Apr 18 18:01:41 2019: iter 200/250  loss 0.00726473331451416  running loss 1.283894419670105\n",
      "Thu Apr 18 18:01:42 2019: iter 250/250  loss 0.0017795562744140625  running loss 1.504685401916504\n",
      "Thu Apr 18 18:01:42 2019: Test accuracy for epoch 40: 1.0\n",
      "Thu Apr 18 18:01:42 2019: EPOCH 42 of 100\n",
      "Thu Apr 18 18:01:42 2019: iter 50/250  loss 0.0011014938354492188  running loss 0.26500701904296875\n",
      "Thu Apr 18 18:01:43 2019: iter 100/250  loss 0.0019116401672363281  running loss 0.5110635757446289\n",
      "Thu Apr 18 18:01:43 2019: iter 150/250  loss 0.0041522979736328125  running loss 0.7560806274414062\n",
      "Thu Apr 18 18:01:44 2019: iter 200/250  loss 0.005466461181640625  running loss 0.9272844791412354\n",
      "Thu Apr 18 18:01:45 2019: iter 250/250  loss 0.004488468170166016  running loss 1.3559517860412598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 18:01:45 2019: Test accuracy for epoch 41: 1.0\n",
      "Thu Apr 18 18:01:45 2019: EPOCH 43 of 100\n",
      "Thu Apr 18 18:01:45 2019: iter 50/250  loss 0.0043032169342041016  running loss 0.2075110673904419\n",
      "Thu Apr 18 18:01:46 2019: iter 100/250  loss 0.0008511543273925781  running loss 0.3937809467315674\n",
      "Thu Apr 18 18:01:46 2019: iter 150/250  loss 0.0012335777282714844  running loss 0.5967915058135986\n",
      "Thu Apr 18 18:01:47 2019: iter 200/250  loss 0.022738337516784668  running loss 0.8545222282409668\n",
      "Thu Apr 18 18:01:48 2019: iter 250/250  loss 0.00590062141418457  running loss 1.250901699066162\n",
      "Thu Apr 18 18:01:48 2019: Test accuracy for epoch 42: 1.0\n",
      "Thu Apr 18 18:01:48 2019: EPOCH 44 of 100\n",
      "Thu Apr 18 18:01:48 2019: iter 50/250  loss 0.004730582237243652  running loss 0.22783362865447998\n",
      "Thu Apr 18 18:01:49 2019: iter 100/250  loss 0.012341856956481934  running loss 0.45401835441589355\n",
      "Thu Apr 18 18:01:49 2019: iter 150/250  loss 0.016266942024230957  running loss 0.6601917743682861\n",
      "Thu Apr 18 18:01:50 2019: iter 200/250  loss 0.011629819869995117  running loss 1.0476603507995605\n",
      "Thu Apr 18 18:01:51 2019: iter 250/250  loss 0.0014374256134033203  running loss 1.243544340133667\n",
      "Thu Apr 18 18:01:51 2019: Test accuracy for epoch 43: 1.0\n",
      "Thu Apr 18 18:01:51 2019: EPOCH 45 of 100\n",
      "Thu Apr 18 18:01:51 2019: iter 50/250  loss 0.009571075439453125  running loss 0.20993447303771973\n",
      "Thu Apr 18 18:01:52 2019: iter 100/250  loss 0.006113767623901367  running loss 0.39659786224365234\n",
      "Thu Apr 18 18:01:52 2019: iter 150/250  loss 0.002216815948486328  running loss 0.5784958600997925\n",
      "Thu Apr 18 18:01:53 2019: iter 200/250  loss 0.0031998157501220703  running loss 0.7786965370178223\n",
      "Thu Apr 18 18:01:53 2019: iter 250/250  loss 0.0028171539306640625  running loss 1.0594204664230347\n",
      "Thu Apr 18 18:01:54 2019: Test accuracy for epoch 44: 1.0\n",
      "Thu Apr 18 18:01:54 2019: EPOCH 46 of 100\n",
      "Thu Apr 18 18:01:54 2019: iter 50/250  loss 0.0017654895782470703  running loss 0.36753225326538086\n",
      "Thu Apr 18 18:01:55 2019: iter 100/250  loss 0.0008373260498046875  running loss 0.5672582387924194\n",
      "Thu Apr 18 18:01:55 2019: iter 150/250  loss 0.00620269775390625  running loss 0.7538213729858398\n",
      "Thu Apr 18 18:01:56 2019: iter 200/250  loss 0.000583648681640625  running loss 0.9487220048904419\n",
      "Thu Apr 18 18:01:56 2019: iter 250/250  loss 0.0018281936645507812  running loss 1.1452676057815552\n",
      "Thu Apr 18 18:01:57 2019: Test accuracy for epoch 45: 1.0\n",
      "Thu Apr 18 18:01:57 2019: EPOCH 47 of 100\n",
      "Thu Apr 18 18:01:57 2019: iter 50/250  loss 0.0011768341064453125  running loss 0.21087801456451416\n",
      "Thu Apr 18 18:01:58 2019: iter 100/250  loss 0.005757808685302734  running loss 0.3959081172943115\n",
      "Thu Apr 18 18:01:58 2019: iter 150/250  loss 0.002844095230102539  running loss 0.5443365573883057\n",
      "Thu Apr 18 18:01:59 2019: iter 200/250  loss 0.001107931137084961  running loss 0.732787013053894\n",
      "Thu Apr 18 18:01:59 2019: iter 250/250  loss 0.005532503128051758  running loss 1.200398564338684\n",
      "Thu Apr 18 18:02:00 2019: Test accuracy for epoch 46: 1.0\n",
      "Thu Apr 18 18:02:00 2019: EPOCH 48 of 100\n",
      "Thu Apr 18 18:02:00 2019: iter 50/250  loss 0.0038421154022216797  running loss 0.1681743860244751\n",
      "Thu Apr 18 18:02:01 2019: iter 100/250  loss 0.003153085708618164  running loss 0.3337653875350952\n",
      "Thu Apr 18 18:02:01 2019: iter 150/250  loss 0.006524562835693359  running loss 0.7557357549667358\n",
      "Thu Apr 18 18:02:02 2019: iter 200/250  loss 0.003534555435180664  running loss 0.8907018899917603\n",
      "Thu Apr 18 18:02:02 2019: iter 250/250  loss 0.0012655258178710938  running loss 1.0827786922454834\n",
      "Thu Apr 18 18:02:03 2019: Test accuracy for epoch 47: 1.0\n",
      "Thu Apr 18 18:02:03 2019: EPOCH 49 of 100\n",
      "Thu Apr 18 18:02:03 2019: iter 50/250  loss 0.0004429817199707031  running loss 0.19197940826416016\n",
      "Thu Apr 18 18:02:04 2019: iter 100/250  loss 0.0010194778442382812  running loss 0.37310945987701416\n",
      "Thu Apr 18 18:02:04 2019: iter 150/250  loss 0.00428462028503418  running loss 0.5361224412918091\n",
      "Thu Apr 18 18:02:05 2019: iter 200/250  loss 0.0009281635284423828  running loss 0.9094431400299072\n",
      "Thu Apr 18 18:02:05 2019: iter 250/250  loss 0.0025551319122314453  running loss 1.0736591815948486\n",
      "Thu Apr 18 18:02:06 2019: Test accuracy for epoch 48: 1.0\n",
      "Thu Apr 18 18:02:06 2019: EPOCH 50 of 100\n",
      "Thu Apr 18 18:02:06 2019: iter 50/250  loss 0.0016293525695800781  running loss 0.14621376991271973\n",
      "Thu Apr 18 18:02:07 2019: iter 100/250  loss 0.0010561943054199219  running loss 0.28625452518463135\n",
      "Thu Apr 18 18:02:07 2019: iter 150/250  loss 0.0006346702575683594  running loss 0.6439297199249268\n",
      "Thu Apr 18 18:02:08 2019: iter 200/250  loss 0.003000974655151367  running loss 0.8895953893661499\n",
      "Thu Apr 18 18:02:09 2019: iter 250/250  loss 0.0031135082244873047  running loss 1.062720537185669\n",
      "Thu Apr 18 18:02:09 2019: Test accuracy for epoch 49: 1.0\n",
      "Thu Apr 18 18:02:09 2019: EPOCH 51 of 100\n",
      "Thu Apr 18 18:02:09 2019: iter 50/250  loss 0.006520748138427734  running loss 0.449049711227417\n",
      "Thu Apr 18 18:02:10 2019: iter 100/250  loss 0.0016417503356933594  running loss 0.6058329343795776\n",
      "Thu Apr 18 18:02:10 2019: iter 150/250  loss 0.002637147903442383  running loss 0.7381995916366577\n",
      "Thu Apr 18 18:02:11 2019: iter 200/250  loss 0.0040073394775390625  running loss 0.8895484209060669\n",
      "Thu Apr 18 18:02:11 2019: iter 250/250  loss 0.00585484504699707  running loss 1.0611974000930786\n",
      "Thu Apr 18 18:02:12 2019: Test accuracy for epoch 50: 1.0\n",
      "Thu Apr 18 18:02:12 2019: EPOCH 52 of 100\n",
      "Thu Apr 18 18:02:12 2019: iter 50/250  loss 0.001811981201171875  running loss 0.1124720573425293\n",
      "Thu Apr 18 18:02:13 2019: iter 100/250  loss 0.0010123252868652344  running loss 0.2916116714477539\n",
      "Thu Apr 18 18:02:13 2019: iter 150/250  loss 0.0024144649505615234  running loss 0.4653540849685669\n",
      "Thu Apr 18 18:02:14 2019: iter 200/250  loss 0.002116680145263672  running loss 0.5910977125167847\n",
      "Thu Apr 18 18:02:14 2019: iter 250/250  loss 0.018360137939453125  running loss 0.9917975664138794\n",
      "Thu Apr 18 18:02:15 2019: Test accuracy for epoch 51: 1.0\n",
      "Thu Apr 18 18:02:15 2019: EPOCH 53 of 100\n",
      "Thu Apr 18 18:02:15 2019: iter 50/250  loss 0.0019001960754394531  running loss 0.15499210357666016\n",
      "Thu Apr 18 18:02:16 2019: iter 100/250  loss 0.000652313232421875  running loss 0.30698585510253906\n",
      "Thu Apr 18 18:02:16 2019: iter 150/250  loss 0.0007357597351074219  running loss 0.7652207612991333\n",
      "Thu Apr 18 18:02:17 2019: iter 200/250  loss 0.0006470680236816406  running loss 0.903114914894104\n",
      "Thu Apr 18 18:02:17 2019: iter 250/250  loss 0.0015079975128173828  running loss 1.0168789625167847\n",
      "Thu Apr 18 18:02:18 2019: Test accuracy for epoch 52: 1.0\n",
      "Thu Apr 18 18:02:18 2019: EPOCH 54 of 100\n",
      "Thu Apr 18 18:02:18 2019: iter 50/250  loss 0.0013227462768554688  running loss 0.13959407806396484\n",
      "Thu Apr 18 18:02:19 2019: iter 100/250  loss 0.007452487945556641  running loss 0.26497960090637207\n",
      "Thu Apr 18 18:02:19 2019: iter 150/250  loss 0.0012748241424560547  running loss 0.4018852710723877\n",
      "Thu Apr 18 18:02:20 2019: iter 200/250  loss 0.0015749931335449219  running loss 0.5408382415771484\n",
      "Thu Apr 18 18:02:20 2019: iter 250/250  loss 0.007534027099609375  running loss 0.9852392673492432\n",
      "Thu Apr 18 18:02:21 2019: Test accuracy for epoch 53: 1.0\n",
      "Thu Apr 18 18:02:21 2019: EPOCH 55 of 100\n",
      "Thu Apr 18 18:02:21 2019: iter 50/250  loss 0.003747224807739258  running loss 0.13885712623596191\n",
      "Thu Apr 18 18:02:22 2019: iter 100/250  loss 0.002042531967163086  running loss 0.29124557971954346\n",
      "Thu Apr 18 18:02:22 2019: iter 150/250  loss 0.0005292892456054688  running loss 0.4384397268295288\n",
      "Thu Apr 18 18:02:23 2019: iter 200/250  loss 0.0005626678466796875  running loss 0.5514853000640869\n",
      "Thu Apr 18 18:02:23 2019: iter 250/250  loss 0.002419710159301758  running loss 1.0444597005844116\n",
      "Thu Apr 18 18:02:24 2019: Test accuracy for epoch 54: 1.0\n",
      "Thu Apr 18 18:02:24 2019: EPOCH 56 of 100\n",
      "Thu Apr 18 18:02:24 2019: iter 50/250  loss 0.0007853507995605469  running loss 0.12258672714233398\n",
      "Thu Apr 18 18:02:25 2019: iter 100/250  loss 0.0011386871337890625  running loss 0.25634002685546875\n",
      "Thu Apr 18 18:02:25 2019: iter 150/250  loss 0.0004363059997558594  running loss 0.39351165294647217\n",
      "Thu Apr 18 18:02:26 2019: iter 200/250  loss 0.0011777877807617188  running loss 0.5200620889663696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 18:02:26 2019: iter 250/250  loss 0.0008101463317871094  running loss 0.8908646106719971\n",
      "Thu Apr 18 18:02:27 2019: Test accuracy for epoch 55: 1.0\n",
      "Thu Apr 18 18:02:27 2019: EPOCH 57 of 100\n",
      "Thu Apr 18 18:02:27 2019: iter 50/250  loss 0.0011904239654541016  running loss 0.12647223472595215\n",
      "Thu Apr 18 18:02:28 2019: iter 100/250  loss 0.0005021095275878906  running loss 0.23830246925354004\n",
      "Thu Apr 18 18:02:28 2019: iter 150/250  loss 0.0014193058013916016  running loss 0.5791858434677124\n",
      "Thu Apr 18 18:02:29 2019: iter 200/250  loss 0.0011472702026367188  running loss 0.7084864377975464\n",
      "Thu Apr 18 18:02:30 2019: iter 250/250  loss 0.001474618911743164  running loss 0.8450053930282593\n",
      "Thu Apr 18 18:02:30 2019: Test accuracy for epoch 56: 1.0\n",
      "Thu Apr 18 18:02:30 2019: EPOCH 58 of 100\n",
      "Thu Apr 18 18:02:31 2019: iter 50/250  loss 0.001054525375366211  running loss 0.11716365814208984\n",
      "Thu Apr 18 18:02:31 2019: iter 100/250  loss 0.00411534309387207  running loss 0.2666299343109131\n",
      "Thu Apr 18 18:02:32 2019: iter 150/250  loss 0.000690460205078125  running loss 0.37897610664367676\n",
      "Thu Apr 18 18:02:32 2019: iter 200/250  loss 0.000946044921875  running loss 0.4966449737548828\n",
      "Thu Apr 18 18:02:33 2019: iter 250/250  loss 0.0008797645568847656  running loss 0.8238047361373901\n",
      "Thu Apr 18 18:02:33 2019: Test accuracy for epoch 57: 1.0\n",
      "Thu Apr 18 18:02:33 2019: EPOCH 59 of 100\n",
      "Thu Apr 18 18:02:34 2019: iter 50/250  loss 0.0029125213623046875  running loss 0.12371468544006348\n",
      "Thu Apr 18 18:02:34 2019: iter 100/250  loss 0.0034551620483398438  running loss 0.23945021629333496\n",
      "Thu Apr 18 18:02:35 2019: iter 150/250  loss 0.0015764236450195312  running loss 0.6337192058563232\n",
      "Thu Apr 18 18:02:35 2019: iter 200/250  loss 0.0012004375457763672  running loss 0.7902805805206299\n",
      "Thu Apr 18 18:02:36 2019: iter 250/250  loss 0.0007092952728271484  running loss 0.8687691688537598\n",
      "Thu Apr 18 18:02:36 2019: Test accuracy for epoch 58: 1.0\n",
      "Thu Apr 18 18:02:36 2019: EPOCH 60 of 100\n",
      "Thu Apr 18 18:02:37 2019: iter 50/250  loss 0.0007352828979492188  running loss 0.11526083946228027\n",
      "Thu Apr 18 18:02:37 2019: iter 100/250  loss 0.00030040740966796875  running loss 0.4579799175262451\n",
      "Thu Apr 18 18:02:38 2019: iter 150/250  loss 0.0009527206420898438  running loss 0.5912330150604248\n",
      "Thu Apr 18 18:02:39 2019: iter 200/250  loss 0.0020110607147216797  running loss 0.7162485122680664\n",
      "Thu Apr 18 18:02:39 2019: iter 250/250  loss 0.001161336898803711  running loss 0.8173196315765381\n",
      "Thu Apr 18 18:02:40 2019: Test accuracy for epoch 59: 1.0\n",
      "Thu Apr 18 18:02:40 2019: EPOCH 61 of 100\n",
      "Thu Apr 18 18:02:40 2019: iter 50/250  loss 0.0005910396575927734  running loss 0.11568164825439453\n",
      "Thu Apr 18 18:02:41 2019: iter 100/250  loss 0.0008096694946289062  running loss 0.23516345024108887\n",
      "Thu Apr 18 18:02:41 2019: iter 150/250  loss 0.0011157989501953125  running loss 0.6562225818634033\n",
      "Thu Apr 18 18:02:42 2019: iter 200/250  loss 0.0005192756652832031  running loss 0.7628958225250244\n",
      "Thu Apr 18 18:02:42 2019: iter 250/250  loss 0.0012958049774169922  running loss 0.8742725849151611\n",
      "Thu Apr 18 18:02:43 2019: Test accuracy for epoch 60: 1.0\n",
      "Thu Apr 18 18:02:43 2019: EPOCH 62 of 100\n",
      "Thu Apr 18 18:02:43 2019: iter 50/250  loss 0.0010542869567871094  running loss 0.37292277812957764\n",
      "Thu Apr 18 18:02:44 2019: iter 100/250  loss 0.0003535747528076172  running loss 0.47770774364471436\n",
      "Thu Apr 18 18:02:44 2019: iter 150/250  loss 0.0012061595916748047  running loss 0.593316912651062\n",
      "Thu Apr 18 18:02:45 2019: iter 200/250  loss 0.0004401206970214844  running loss 0.7075053453445435\n",
      "Thu Apr 18 18:02:45 2019: iter 250/250  loss 0.0024018287658691406  running loss 0.8227452039718628\n",
      "Thu Apr 18 18:02:46 2019: Test accuracy for epoch 61: 1.0\n",
      "Thu Apr 18 18:02:46 2019: EPOCH 63 of 100\n",
      "Thu Apr 18 18:02:46 2019: iter 50/250  loss 0.002626657485961914  running loss 0.11203527450561523\n",
      "Thu Apr 18 18:02:47 2019: iter 100/250  loss 0.0006890296936035156  running loss 0.21805310249328613\n",
      "Thu Apr 18 18:02:47 2019: iter 150/250  loss 0.004830837249755859  running loss 0.5837275981903076\n",
      "Thu Apr 18 18:02:48 2019: iter 200/250  loss 0.0032701492309570312  running loss 0.6781690120697021\n",
      "Thu Apr 18 18:02:48 2019: iter 250/250  loss 0.0013768672943115234  running loss 0.7836441993713379\n",
      "Thu Apr 18 18:02:49 2019: Test accuracy for epoch 62: 1.0\n",
      "Thu Apr 18 18:02:49 2019: EPOCH 64 of 100\n",
      "Thu Apr 18 18:02:49 2019: iter 50/250  loss 0.00048542022705078125  running loss 0.35983848571777344\n",
      "Thu Apr 18 18:02:50 2019: iter 100/250  loss 0.005329608917236328  running loss 0.45767831802368164\n",
      "Thu Apr 18 18:02:50 2019: iter 150/250  loss 0.0037403106689453125  running loss 0.5685009956359863\n",
      "Thu Apr 18 18:02:51 2019: iter 200/250  loss 0.000560760498046875  running loss 0.6723604202270508\n",
      "Thu Apr 18 18:02:51 2019: iter 250/250  loss 0.0006289482116699219  running loss 0.7797839641571045\n",
      "Thu Apr 18 18:02:52 2019: Test accuracy for epoch 63: 1.0\n",
      "Thu Apr 18 18:02:52 2019: EPOCH 65 of 100\n",
      "Thu Apr 18 18:02:52 2019: iter 50/250  loss 0.002534151077270508  running loss 0.09544587135314941\n",
      "Thu Apr 18 18:02:53 2019: iter 100/250  loss 0.001340627670288086  running loss 0.17436599731445312\n",
      "Thu Apr 18 18:02:53 2019: iter 150/250  loss 0.00915670394897461  running loss 0.44122231006622314\n",
      "Thu Apr 18 18:02:54 2019: iter 200/250  loss 0.0006351470947265625  running loss 0.6331135034561157\n",
      "Thu Apr 18 18:02:54 2019: iter 250/250  loss 0.00042510032653808594  running loss 0.7338961362838745\n",
      "Thu Apr 18 18:02:55 2019: Test accuracy for epoch 64: 1.0\n",
      "Thu Apr 18 18:02:55 2019: EPOCH 66 of 100\n",
      "Thu Apr 18 18:02:55 2019: iter 50/250  loss 0.002220630645751953  running loss 0.346423864364624\n",
      "Thu Apr 18 18:02:56 2019: iter 100/250  loss 0.00021457672119140625  running loss 0.43280816078186035\n",
      "Thu Apr 18 18:02:56 2019: iter 150/250  loss 0.0009794235229492188  running loss 0.5484097003936768\n",
      "Thu Apr 18 18:02:57 2019: iter 200/250  loss 0.0011332035064697266  running loss 0.6508421897888184\n",
      "Thu Apr 18 18:02:57 2019: iter 250/250  loss 0.008332967758178711  running loss 0.7496404647827148\n",
      "Thu Apr 18 18:02:58 2019: Test accuracy for epoch 65: 1.0\n",
      "Thu Apr 18 18:02:58 2019: EPOCH 67 of 100\n",
      "Thu Apr 18 18:02:58 2019: iter 50/250  loss 0.0065615177154541016  running loss 0.11143708229064941\n",
      "Thu Apr 18 18:02:59 2019: iter 100/250  loss 0.0013701915740966797  running loss 0.20840907096862793\n",
      "Thu Apr 18 18:02:59 2019: iter 150/250  loss 0.0013501644134521484  running loss 0.5473711490631104\n",
      "Thu Apr 18 18:03:00 2019: iter 200/250  loss 0.004436969757080078  running loss 0.6248133182525635\n",
      "Thu Apr 18 18:03:00 2019: iter 250/250  loss 0.0008037090301513672  running loss 0.7161104679107666\n",
      "Thu Apr 18 18:03:01 2019: Test accuracy for epoch 66: 1.0\n",
      "Thu Apr 18 18:03:01 2019: EPOCH 68 of 100\n",
      "Thu Apr 18 18:03:01 2019: iter 50/250  loss 0.00545954704284668  running loss 0.3303258419036865\n",
      "Thu Apr 18 18:03:02 2019: iter 100/250  loss 5.340576171875e-05  running loss 0.4197366237640381\n",
      "Thu Apr 18 18:03:02 2019: iter 150/250  loss 0.001954793930053711  running loss 0.49974846839904785\n",
      "Thu Apr 18 18:03:03 2019: iter 200/250  loss 0.0008003711700439453  running loss 0.5794885158538818\n",
      "Thu Apr 18 18:03:03 2019: iter 250/250  loss 0.0004630088806152344  running loss 0.6890287399291992\n",
      "Thu Apr 18 18:03:04 2019: Test accuracy for epoch 67: 1.0\n",
      "Thu Apr 18 18:03:04 2019: EPOCH 69 of 100\n",
      "Thu Apr 18 18:03:04 2019: iter 50/250  loss 0.0007722377777099609  running loss 0.10215401649475098\n",
      "Thu Apr 18 18:03:05 2019: iter 100/250  loss 0.0016417503356933594  running loss 0.4300142526626587\n",
      "Thu Apr 18 18:03:05 2019: iter 150/250  loss 0.0005710124969482422  running loss 0.5073860883712769\n",
      "Thu Apr 18 18:03:06 2019: iter 200/250  loss 0.0006365776062011719  running loss 0.6129480600357056\n",
      "Thu Apr 18 18:03:06 2019: iter 250/250  loss 0.0012905597686767578  running loss 0.7026952505111694\n",
      "Thu Apr 18 18:03:07 2019: Test accuracy for epoch 68: 1.0\n",
      "Thu Apr 18 18:03:07 2019: EPOCH 70 of 100\n",
      "Thu Apr 18 18:03:07 2019: iter 50/250  loss 0.0012242794036865234  running loss 0.08776593208312988\n",
      "Thu Apr 18 18:03:08 2019: iter 100/250  loss 0.00043010711669921875  running loss 0.18372321128845215\n",
      "Thu Apr 18 18:03:09 2019: iter 150/250  loss 0.0016345977783203125  running loss 0.2774043083190918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 18:03:09 2019: iter 200/250  loss 0.0008141994476318359  running loss 0.35329437255859375\n",
      "Thu Apr 18 18:03:10 2019: iter 250/250  loss 0.0008668899536132812  running loss 0.6373945474624634\n",
      "Thu Apr 18 18:03:10 2019: Test accuracy for epoch 69: 1.0\n",
      "Thu Apr 18 18:03:10 2019: EPOCH 71 of 100\n",
      "Thu Apr 18 18:03:11 2019: iter 50/250  loss 0.0003750324249267578  running loss 0.29145145416259766\n",
      "Thu Apr 18 18:03:11 2019: iter 100/250  loss 0.001971006393432617  running loss 0.3854668140411377\n",
      "Thu Apr 18 18:03:12 2019: iter 150/250  loss 0.0021085739135742188  running loss 0.49733424186706543\n",
      "Thu Apr 18 18:03:13 2019: iter 200/250  loss 0.0006787776947021484  running loss 0.5790929794311523\n",
      "Thu Apr 18 18:03:13 2019: iter 250/250  loss 0.0015070438385009766  running loss 0.6510272026062012\n",
      "Thu Apr 18 18:03:13 2019: Test accuracy for epoch 70: 1.0\n",
      "Thu Apr 18 18:03:13 2019: EPOCH 72 of 100\n",
      "Thu Apr 18 18:03:14 2019: iter 50/250  loss 0.002580881118774414  running loss 0.08542013168334961\n",
      "Thu Apr 18 18:03:15 2019: iter 100/250  loss 0.0008490085601806641  running loss 0.15669894218444824\n",
      "Thu Apr 18 18:03:15 2019: iter 150/250  loss 0.00109100341796875  running loss 0.5168496370315552\n",
      "Thu Apr 18 18:03:16 2019: iter 200/250  loss 0.0033173561096191406  running loss 0.6171272993087769\n",
      "Thu Apr 18 18:03:16 2019: iter 250/250  loss 0.002440214157104492  running loss 0.6837564706802368\n",
      "Thu Apr 18 18:03:17 2019: Test accuracy for epoch 71: 1.0\n",
      "Thu Apr 18 18:03:17 2019: EPOCH 73 of 100\n",
      "Thu Apr 18 18:03:17 2019: iter 50/250  loss 0.0003781318664550781  running loss 0.06950712203979492\n",
      "Thu Apr 18 18:03:18 2019: iter 100/250  loss 0.0012657642364501953  running loss 0.17157745361328125\n",
      "Thu Apr 18 18:03:18 2019: iter 150/250  loss 0.0007524490356445312  running loss 0.24755454063415527\n",
      "Thu Apr 18 18:03:19 2019: iter 200/250  loss 0.00017833709716796875  running loss 0.3262770175933838\n",
      "Thu Apr 18 18:03:19 2019: iter 250/250  loss 0.002214670181274414  running loss 0.5766133069992065\n",
      "Thu Apr 18 18:03:20 2019: Test accuracy for epoch 72: 1.0\n",
      "Thu Apr 18 18:03:20 2019: EPOCH 74 of 100\n",
      "Thu Apr 18 18:03:20 2019: iter 50/250  loss 0.00201416015625  running loss 0.09403324127197266\n",
      "Thu Apr 18 18:03:21 2019: iter 100/250  loss 0.0007402896881103516  running loss 0.18271613121032715\n",
      "Thu Apr 18 18:03:21 2019: iter 150/250  loss 0.002651214599609375  running loss 0.25949907302856445\n",
      "Thu Apr 18 18:03:22 2019: iter 200/250  loss 0.0032949447631835938  running loss 0.3204176425933838\n",
      "Thu Apr 18 18:03:23 2019: iter 250/250  loss 0.0009398460388183594  running loss 0.6772564649581909\n",
      "Thu Apr 18 18:03:23 2019: Test accuracy for epoch 73: 1.0\n",
      "Thu Apr 18 18:03:23 2019: EPOCH 75 of 100\n",
      "Thu Apr 18 18:03:24 2019: iter 50/250  loss 0.0004563331604003906  running loss 0.06743240356445312\n",
      "Thu Apr 18 18:03:24 2019: iter 100/250  loss 0.001596212387084961  running loss 0.13517403602600098\n",
      "Thu Apr 18 18:03:25 2019: iter 150/250  loss 0.0025386810302734375  running loss 0.21532845497131348\n",
      "Thu Apr 18 18:03:25 2019: iter 200/250  loss 0.0007147789001464844  running loss 0.30235791206359863\n",
      "Thu Apr 18 18:03:26 2019: iter 250/250  loss 0.0028107166290283203  running loss 0.5419940948486328\n",
      "Thu Apr 18 18:03:26 2019: Test accuracy for epoch 74: 1.0\n",
      "Thu Apr 18 18:03:26 2019: EPOCH 76 of 100\n",
      "Thu Apr 18 18:03:27 2019: iter 50/250  loss 0.001129150390625  running loss 0.08501863479614258\n",
      "Thu Apr 18 18:03:27 2019: iter 100/250  loss 0.006346464157104492  running loss 0.3768042325973511\n",
      "Thu Apr 18 18:03:28 2019: iter 150/250  loss 0.006478309631347656  running loss 0.4526442289352417\n",
      "Thu Apr 18 18:03:29 2019: iter 200/250  loss 0.0015099048614501953  running loss 0.5266414880752563\n",
      "Thu Apr 18 18:03:29 2019: iter 250/250  loss 0.0002562999725341797  running loss 0.6032134294509888\n",
      "Thu Apr 18 18:03:29 2019: Test accuracy for epoch 75: 1.0\n",
      "Thu Apr 18 18:03:29 2019: EPOCH 77 of 100\n",
      "Thu Apr 18 18:03:30 2019: iter 50/250  loss 0.0008084774017333984  running loss 0.06564092636108398\n",
      "Thu Apr 18 18:03:31 2019: iter 100/250  loss 0.0009315013885498047  running loss 0.13577628135681152\n",
      "Thu Apr 18 18:03:32 2019: iter 150/250  loss 0.0006105899810791016  running loss 0.2059786319732666\n",
      "Thu Apr 18 18:03:32 2019: iter 200/250  loss 0.0010302066802978516  running loss 0.5416840314865112\n",
      "Thu Apr 18 18:03:33 2019: iter 250/250  loss 0.0004909038543701172  running loss 0.6286884546279907\n",
      "Thu Apr 18 18:03:33 2019: Test accuracy for epoch 76: 1.0\n",
      "Thu Apr 18 18:03:33 2019: EPOCH 78 of 100\n",
      "Thu Apr 18 18:03:34 2019: iter 50/250  loss 0.0005507469177246094  running loss 0.28383827209472656\n",
      "Thu Apr 18 18:03:35 2019: iter 100/250  loss 0.0004944801330566406  running loss 0.3716435432434082\n",
      "Thu Apr 18 18:03:35 2019: iter 150/250  loss 0.0010182857513427734  running loss 0.4405043125152588\n",
      "Thu Apr 18 18:03:36 2019: iter 200/250  loss 0.00047969818115234375  running loss 0.5017471313476562\n",
      "Thu Apr 18 18:03:36 2019: iter 250/250  loss 0.002110719680786133  running loss 0.5898675918579102\n",
      "Thu Apr 18 18:03:37 2019: Test accuracy for epoch 77: 1.0\n",
      "Thu Apr 18 18:03:37 2019: EPOCH 79 of 100\n",
      "Thu Apr 18 18:03:37 2019: iter 50/250  loss 0.0019428730010986328  running loss 0.07957625389099121\n",
      "Thu Apr 18 18:03:38 2019: iter 100/250  loss 0.0005867481231689453  running loss 0.1397392749786377\n",
      "Thu Apr 18 18:03:38 2019: iter 150/250  loss 0.0008051395416259766  running loss 0.21244335174560547\n",
      "Thu Apr 18 18:03:39 2019: iter 200/250  loss 0.0008325576782226562  running loss 0.2723526954650879\n",
      "Thu Apr 18 18:03:40 2019: iter 250/250  loss 0.00032520294189453125  running loss 0.5959740877151489\n",
      "Thu Apr 18 18:03:40 2019: Test accuracy for epoch 78: 1.0\n",
      "Thu Apr 18 18:03:40 2019: EPOCH 80 of 100\n",
      "Thu Apr 18 18:03:40 2019: iter 50/250  loss 0.0004858970642089844  running loss 0.05370473861694336\n",
      "Thu Apr 18 18:03:41 2019: iter 100/250  loss 0.0017604827880859375  running loss 0.39456403255462646\n",
      "Thu Apr 18 18:03:42 2019: iter 150/250  loss 0.007014274597167969  running loss 0.4783421754837036\n",
      "Thu Apr 18 18:03:42 2019: iter 200/250  loss 0.002556324005126953  running loss 0.5466541051864624\n",
      "Thu Apr 18 18:03:43 2019: iter 250/250  loss 0.0004513263702392578  running loss 0.6202939748764038\n",
      "Thu Apr 18 18:03:43 2019: Test accuracy for epoch 79: 1.0\n",
      "Thu Apr 18 18:03:43 2019: EPOCH 81 of 100\n",
      "Thu Apr 18 18:03:44 2019: iter 50/250  loss 0.0005106925964355469  running loss 0.32043683528900146\n",
      "Thu Apr 18 18:03:44 2019: iter 100/250  loss 0.001317739486694336  running loss 0.3953598737716675\n",
      "Thu Apr 18 18:03:45 2019: iter 150/250  loss 0.008913278579711914  running loss 0.4612957239151001\n",
      "Thu Apr 18 18:03:45 2019: iter 200/250  loss 0.0006375312805175781  running loss 0.539925217628479\n",
      "Thu Apr 18 18:03:46 2019: iter 250/250  loss 0.0007920265197753906  running loss 0.6106868982315063\n",
      "Thu Apr 18 18:03:46 2019: Test accuracy for epoch 80: 1.0\n",
      "Thu Apr 18 18:03:46 2019: EPOCH 82 of 100\n",
      "Thu Apr 18 18:03:47 2019: iter 50/250  loss 0.0009360313415527344  running loss 0.33991217613220215\n",
      "Thu Apr 18 18:03:47 2019: iter 100/250  loss 0.0002651214599609375  running loss 0.39568042755126953\n",
      "Thu Apr 18 18:03:48 2019: iter 150/250  loss 0.0006198883056640625  running loss 0.4678528308868408\n",
      "Thu Apr 18 18:03:48 2019: iter 200/250  loss 0.00164031982421875  running loss 0.5210041999816895\n",
      "Thu Apr 18 18:03:49 2019: iter 250/250  loss 0.001911163330078125  running loss 0.5976157188415527\n",
      "Thu Apr 18 18:03:49 2019: Test accuracy for epoch 81: 1.0\n",
      "Thu Apr 18 18:03:49 2019: EPOCH 83 of 100\n",
      "Thu Apr 18 18:03:50 2019: iter 50/250  loss 0.0005781650543212891  running loss 0.06050896644592285\n",
      "Thu Apr 18 18:03:50 2019: iter 100/250  loss 0.002608060836791992  running loss 0.12759041786193848\n",
      "Thu Apr 18 18:03:51 2019: iter 150/250  loss 0.001483917236328125  running loss 0.18887805938720703\n",
      "Thu Apr 18 18:03:52 2019: iter 200/250  loss 0.0004773139953613281  running loss 0.2552306652069092\n",
      "Thu Apr 18 18:03:52 2019: iter 250/250  loss 0.0011148452758789062  running loss 0.6578389406204224\n",
      "Thu Apr 18 18:03:52 2019: Test accuracy for epoch 82: 1.0\n",
      "Thu Apr 18 18:03:52 2019: EPOCH 84 of 100\n",
      "Thu Apr 18 18:03:53 2019: iter 50/250  loss 0.0006725788116455078  running loss 0.061499834060668945\n",
      "Thu Apr 18 18:03:53 2019: iter 100/250  loss 0.0008158683776855469  running loss 0.3050549030303955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 18:03:54 2019: iter 150/250  loss 0.00220489501953125  running loss 0.3856689929962158\n",
      "Thu Apr 18 18:03:55 2019: iter 200/250  loss 0.002689361572265625  running loss 0.4281938076019287\n",
      "Thu Apr 18 18:03:55 2019: iter 250/250  loss 0.0003113746643066406  running loss 0.5001001358032227\n",
      "Thu Apr 18 18:03:55 2019: Test accuracy for epoch 83: 1.0\n",
      "Thu Apr 18 18:03:55 2019: EPOCH 85 of 100\n",
      "Thu Apr 18 18:03:56 2019: iter 50/250  loss 0.0008363723754882812  running loss 0.0663151741027832\n",
      "Thu Apr 18 18:03:57 2019: iter 100/250  loss 0.0037560462951660156  running loss 0.13071632385253906\n",
      "Thu Apr 18 18:03:57 2019: iter 150/250  loss 0.00048732757568359375  running loss 0.19929242134094238\n",
      "Thu Apr 18 18:03:58 2019: iter 200/250  loss 0.00016021728515625  running loss 0.2554435729980469\n",
      "Thu Apr 18 18:03:58 2019: iter 250/250  loss 0.0005936622619628906  running loss 0.47195863723754883\n",
      "Thu Apr 18 18:03:59 2019: Test accuracy for epoch 84: 1.0\n",
      "Thu Apr 18 18:03:59 2019: EPOCH 86 of 100\n",
      "Thu Apr 18 18:03:59 2019: iter 50/250  loss 0.003427743911743164  running loss 0.1386721134185791\n",
      "Thu Apr 18 18:04:00 2019: iter 100/250  loss 0.000324249267578125  running loss 0.4210388660430908\n",
      "Thu Apr 18 18:04:00 2019: iter 150/250  loss 0.0008006095886230469  running loss 0.47779083251953125\n",
      "Thu Apr 18 18:04:01 2019: iter 200/250  loss 0.0003094673156738281  running loss 0.5489578247070312\n",
      "Thu Apr 18 18:04:01 2019: iter 250/250  loss 0.0010843276977539062  running loss 0.6119132041931152\n",
      "Thu Apr 18 18:04:02 2019: Test accuracy for epoch 85: 1.0\n",
      "Thu Apr 18 18:04:02 2019: EPOCH 87 of 100\n",
      "Thu Apr 18 18:04:02 2019: iter 50/250  loss 0.0007886886596679688  running loss 0.06441211700439453\n",
      "Thu Apr 18 18:04:03 2019: iter 100/250  loss 0.00107574462890625  running loss 0.12343788146972656\n",
      "Thu Apr 18 18:04:03 2019: iter 150/250  loss 0.0002956390380859375  running loss 0.19086170196533203\n",
      "Thu Apr 18 18:04:04 2019: iter 200/250  loss 0.0012073516845703125  running loss 0.45284509658813477\n",
      "Thu Apr 18 18:04:05 2019: iter 250/250  loss 0.0018799304962158203  running loss 0.520240306854248\n",
      "Thu Apr 18 18:04:05 2019: Test accuracy for epoch 86: 1.0\n",
      "Thu Apr 18 18:04:05 2019: EPOCH 88 of 100\n",
      "Thu Apr 18 18:04:06 2019: iter 50/250  loss 0.0007255077362060547  running loss 0.293865442276001\n",
      "Thu Apr 18 18:04:06 2019: iter 100/250  loss 0.0011987686157226562  running loss 0.3579273223876953\n",
      "Thu Apr 18 18:04:07 2019: iter 150/250  loss 0.0018310546875  running loss 0.41115283966064453\n",
      "Thu Apr 18 18:04:07 2019: iter 200/250  loss 0.00042438507080078125  running loss 0.4588320255279541\n",
      "Thu Apr 18 18:04:08 2019: iter 250/250  loss 0.0007290840148925781  running loss 0.5172808170318604\n",
      "Thu Apr 18 18:04:08 2019: Test accuracy for epoch 87: 1.0\n",
      "Thu Apr 18 18:04:08 2019: EPOCH 89 of 100\n",
      "Thu Apr 18 18:04:09 2019: iter 50/250  loss 0.0004820823669433594  running loss 0.04833650588989258\n",
      "Thu Apr 18 18:04:09 2019: iter 100/250  loss 0.0013201236724853516  running loss 0.10786962509155273\n",
      "Thu Apr 18 18:04:10 2019: iter 150/250  loss 0.0003509521484375  running loss 0.16812682151794434\n",
      "Thu Apr 18 18:04:10 2019: iter 200/250  loss 0.0008087158203125  running loss 0.4597361087799072\n",
      "Thu Apr 18 18:04:11 2019: iter 250/250  loss 0.00026869773864746094  running loss 0.5180258750915527\n",
      "Thu Apr 18 18:04:11 2019: Test accuracy for epoch 88: 1.0\n",
      "Thu Apr 18 18:04:11 2019: EPOCH 90 of 100\n",
      "Thu Apr 18 18:04:12 2019: iter 50/250  loss 0.0003867149353027344  running loss 0.07115602493286133\n",
      "Thu Apr 18 18:04:12 2019: iter 100/250  loss 0.00028705596923828125  running loss 0.3702511787414551\n",
      "Thu Apr 18 18:04:13 2019: iter 150/250  loss 0.0007798671722412109  running loss 0.43403172492980957\n",
      "Thu Apr 18 18:04:13 2019: iter 200/250  loss 0.0010538101196289062  running loss 0.4812197685241699\n",
      "Thu Apr 18 18:04:14 2019: iter 250/250  loss 0.0020623207092285156  running loss 0.546475887298584\n",
      "Thu Apr 18 18:04:15 2019: Test accuracy for epoch 89: 1.0\n",
      "Thu Apr 18 18:04:15 2019: EPOCH 91 of 100\n",
      "Thu Apr 18 18:04:15 2019: iter 50/250  loss 0.000217437744140625  running loss 0.05256295204162598\n",
      "Thu Apr 18 18:04:16 2019: iter 100/250  loss 0.0005869865417480469  running loss 0.3045642375946045\n",
      "Thu Apr 18 18:04:16 2019: iter 150/250  loss 0.0005297660827636719  running loss 0.36812686920166016\n",
      "Thu Apr 18 18:04:17 2019: iter 200/250  loss 0.00047850608825683594  running loss 0.41858601570129395\n",
      "Thu Apr 18 18:04:17 2019: iter 250/250  loss 0.00022125244140625  running loss 0.47421908378601074\n",
      "Thu Apr 18 18:04:18 2019: Test accuracy for epoch 90: 1.0\n",
      "Thu Apr 18 18:04:18 2019: EPOCH 92 of 100\n",
      "Thu Apr 18 18:04:18 2019: iter 50/250  loss 0.0007326602935791016  running loss 0.04555249214172363\n",
      "Thu Apr 18 18:04:19 2019: iter 100/250  loss 0.0035076141357421875  running loss 0.09801840782165527\n",
      "Thu Apr 18 18:04:19 2019: iter 150/250  loss 0.0002841949462890625  running loss 0.3449137210845947\n",
      "Thu Apr 18 18:04:20 2019: iter 200/250  loss 0.0018210411071777344  running loss 0.4067647457122803\n",
      "Thu Apr 18 18:04:20 2019: iter 250/250  loss 0.0005257129669189453  running loss 0.4749026298522949\n",
      "Thu Apr 18 18:04:21 2019: Test accuracy for epoch 91: 1.0\n",
      "Thu Apr 18 18:04:21 2019: EPOCH 93 of 100\n",
      "Thu Apr 18 18:04:21 2019: iter 50/250  loss 0.0019500255584716797  running loss 0.06727838516235352\n",
      "Thu Apr 18 18:04:22 2019: iter 100/250  loss 0.002941131591796875  running loss 0.3291144371032715\n",
      "Thu Apr 18 18:04:22 2019: iter 150/250  loss 0.00022840499877929688  running loss 0.3746519088745117\n",
      "Thu Apr 18 18:04:23 2019: iter 200/250  loss 0.002114534378051758  running loss 0.4265449047088623\n",
      "Thu Apr 18 18:04:24 2019: iter 250/250  loss 0.0009315013885498047  running loss 0.4743785858154297\n",
      "Thu Apr 18 18:04:24 2019: Test accuracy for epoch 92: 1.0\n",
      "Thu Apr 18 18:04:24 2019: EPOCH 94 of 100\n",
      "Thu Apr 18 18:04:24 2019: iter 50/250  loss 0.00030303001403808594  running loss 0.04320979118347168\n",
      "Thu Apr 18 18:04:25 2019: iter 100/250  loss 0.00039005279541015625  running loss 0.3254811763763428\n",
      "Thu Apr 18 18:04:26 2019: iter 150/250  loss 0.0005447864532470703  running loss 0.37694621086120605\n",
      "Thu Apr 18 18:04:26 2019: iter 200/250  loss 0.0007963180541992188  running loss 0.4192817211151123\n",
      "Thu Apr 18 18:04:27 2019: iter 250/250  loss 0.0011396408081054688  running loss 0.4734370708465576\n",
      "Thu Apr 18 18:04:27 2019: Test accuracy for epoch 93: 1.0\n",
      "Thu Apr 18 18:04:27 2019: EPOCH 95 of 100\n",
      "Thu Apr 18 18:04:27 2019: iter 50/250  loss 0.0026235580444335938  running loss 0.241560697555542\n",
      "Thu Apr 18 18:04:28 2019: iter 100/250  loss 0.0004715919494628906  running loss 0.31014466285705566\n",
      "Thu Apr 18 18:04:29 2019: iter 150/250  loss 0.0018076896667480469  running loss 0.3789217472076416\n",
      "Thu Apr 18 18:04:29 2019: iter 200/250  loss 0.0002536773681640625  running loss 0.4186851978302002\n",
      "Thu Apr 18 18:04:30 2019: iter 250/250  loss 0.0016608238220214844  running loss 0.4867215156555176\n",
      "Thu Apr 18 18:04:30 2019: Test accuracy for epoch 94: 1.0\n",
      "Thu Apr 18 18:04:30 2019: EPOCH 96 of 100\n",
      "Thu Apr 18 18:04:31 2019: iter 50/250  loss 0.0004048347473144531  running loss 0.052187204360961914\n",
      "Thu Apr 18 18:04:31 2019: iter 100/250  loss 0.00029158592224121094  running loss 0.09304666519165039\n",
      "Thu Apr 18 18:04:32 2019: iter 150/250  loss 0.0015892982482910156  running loss 0.14588499069213867\n",
      "Thu Apr 18 18:04:33 2019: iter 200/250  loss 0.001987457275390625  running loss 0.2017683982849121\n",
      "Thu Apr 18 18:04:33 2019: iter 250/250  loss 0.0003974437713623047  running loss 0.44800353050231934\n",
      "Thu Apr 18 18:04:34 2019: Test accuracy for epoch 95: 1.0\n",
      "Thu Apr 18 18:04:34 2019: EPOCH 97 of 100\n",
      "Thu Apr 18 18:04:34 2019: iter 50/250  loss 0.001505136489868164  running loss 0.04838919639587402\n",
      "Thu Apr 18 18:04:35 2019: iter 100/250  loss 0.00054931640625  running loss 0.270402193069458\n",
      "Thu Apr 18 18:04:35 2019: iter 150/250  loss 0.0016448497772216797  running loss 0.3307948112487793\n",
      "Thu Apr 18 18:04:36 2019: iter 200/250  loss 0.000858306884765625  running loss 0.38373470306396484\n",
      "Thu Apr 18 18:04:36 2019: iter 250/250  loss 0.003778696060180664  running loss 0.42943310737609863\n",
      "Thu Apr 18 18:04:37 2019: Test accuracy for epoch 96: 1.0\n",
      "Thu Apr 18 18:04:37 2019: EPOCH 98 of 100\n",
      "Thu Apr 18 18:04:37 2019: iter 50/250  loss 0.001148223876953125  running loss 0.23403716087341309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 18 18:04:38 2019: iter 100/250  loss 0.005831718444824219  running loss 0.29160046577453613\n",
      "Thu Apr 18 18:04:38 2019: iter 150/250  loss 0.0035860538482666016  running loss 0.35128211975097656\n",
      "Thu Apr 18 18:04:39 2019: iter 200/250  loss 0.00045609474182128906  running loss 0.3998434543609619\n",
      "Thu Apr 18 18:04:39 2019: iter 250/250  loss 0.0009102821350097656  running loss 0.44585561752319336\n",
      "Thu Apr 18 18:04:40 2019: Test accuracy for epoch 97: 1.0\n",
      "Thu Apr 18 18:04:40 2019: EPOCH 99 of 100\n",
      "Thu Apr 18 18:04:40 2019: iter 50/250  loss 0.0001316070556640625  running loss 0.04654240608215332\n",
      "Thu Apr 18 18:04:41 2019: iter 100/250  loss 0.0013399124145507812  running loss 0.0926353931427002\n",
      "Thu Apr 18 18:04:41 2019: iter 150/250  loss 0.00042819976806640625  running loss 0.13300108909606934\n",
      "Thu Apr 18 18:04:42 2019: iter 200/250  loss 0.0003333091735839844  running loss 0.19655680656433105\n",
      "Thu Apr 18 18:04:42 2019: iter 250/250  loss 0.0012197494506835938  running loss 0.43508315086364746\n",
      "Thu Apr 18 18:04:42 2019: Test accuracy for epoch 98: 1.0\n",
      "Thu Apr 18 18:04:42 2019: EPOCH 100 of 100\n",
      "Thu Apr 18 18:04:43 2019: iter 50/250  loss 0.013931751251220703  running loss 0.16735315322875977\n",
      "Thu Apr 18 18:04:44 2019: iter 100/250  loss 0.0009872913360595703  running loss 0.2626307010650635\n",
      "Thu Apr 18 18:04:44 2019: iter 150/250  loss 0.0012660026550292969  running loss 0.30646204948425293\n",
      "Thu Apr 18 18:04:45 2019: iter 200/250  loss 0.00044274330139160156  running loss 0.35024142265319824\n",
      "Thu Apr 18 18:04:45 2019: iter 250/250  loss 0.0015556812286376953  running loss 0.4103822708129883\n",
      "Thu Apr 18 18:04:45 2019: Test accuracy for epoch 99: 1.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    tlog('EPOCH {} of {}'.format(epoch + 1, N_EPOCHS))\n",
    "    \n",
    "    # train\n",
    "    running_loss = 0.\n",
    "    for i, (images, labels) in enumerate(training_loader):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        guesses, _, _ = model(images) # discard intermediate outputs\n",
    "        loss = loss_fn(guesses, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            tlog('iter {}/{}  loss {}  running loss {}'.format(i + 1, len(training_dataset) // BATCH_SIZE, loss.item(), running_loss))\n",
    "    \n",
    "    # validate\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(testing_loader):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            guesses, _, _ = model(images)\n",
    "            _, predictions = torch.max(guesses, 1)\n",
    "            total += labels.size(0) # add batch size to total\n",
    "            correct += (predictions == labels).long().sum().item()\n",
    "    tlog('Test accuracy for epoch {}: {}'.format(epoch, float(correct) / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = make_training_feature1(input0()) # depends on global fns oops\n",
    "tf2 = make_training_feature2(input0())\n",
    "tf3 = make_training_feature3(input0())\n",
    "tf4 = make_training_feature4(input0())\n",
    "tfs = [tf1, tf2, tf3, tf4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 0 for 0\n",
      "got 0 for 0\n",
      "got 0 for 0\n",
      "got 0 for 0\n",
      "got 1 for 1\n",
      "got 1 for 1\n",
      "got 1 for 1\n",
      "got 1 for 1\n",
      "got 2 for 2\n",
      "got 2 for 2\n",
      "got 2 for 2\n",
      "got 2 for 2\n",
      "got 3 for 3\n",
      "got 3 for 3\n",
      "got 3 for 3\n",
      "got 3 for 3\n"
     ]
    }
   ],
   "source": [
    "for i, tf_raw in enumerate(tfs):\n",
    "    tf = torch.squeeze(tf_raw)\n",
    "    tfv = reflect_vert(tf)\n",
    "    tfh = reflect_horiz(tf)\n",
    "    tfvh = reflect_horiz(tfv)\n",
    "    for t_in in [tf, tfv, tfh, tfvh]:\n",
    "        t_in = torch.squeeze(add_light_fuzz(torch.unsqueeze(t_in, 0), fuzz_factor=FUZZ_FACTOR))\n",
    "        guess, _, _ = model(torch.unsqueeze(torch.unsqueeze(t_in, 0), 0))\n",
    "        _, pred = torch.max(guess, 1)\n",
    "        print('got {} for {}'.format(pred.item(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_out, t_int, t_conv = model(torch.unsqueeze(input0(), 0))\n",
    "print('output shape {}'.format(t_out.shape))\n",
    "print('post-integration shape {}'.format(t_int.shape))\n",
    "print('post-conv shape {}'.format(t_conv.shape))\n",
    "print(torch.squeeze(t_out, 0))\n",
    "print(torch.squeeze(t_int, 0))\n",
    "visualize_s2conv_out(torch.squeeze(t_conv, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_conv_layer_for(tf1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
