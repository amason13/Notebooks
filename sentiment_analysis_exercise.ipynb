{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis Exercise",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q-DoPGl0j7gc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k16HBOovsZ0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below are some helpers, and a few lines to nudge you to change your Colab runtime to GPU (just in case you hadn't already)."
      ]
    },
    {
      "metadata": {
        "id": "pHWwa-4JlM78",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tlog(msg):\n",
        "    print('{}  {}'.format(time.asctime(), msg))\n",
        "\n",
        "\n",
        "# If possible, we should be running on GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print('If you are running this notebook in Colab, go to the Runtime menu and select \"Change runtime type\" to switch to GPU.')\n",
        "else:\n",
        "    print('GPU ready to go!')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MSjpzwUQsYyx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "teknD8kimZUz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# indices\n",
        "I_PHRASE_ID = 0\n",
        "I_SENTENCE_ID = 1\n",
        "I_PHRASE = 2\n",
        "I_LABEL = 3\n",
        "I_TOKEN_LIST = 4\n",
        "\n",
        "# constants\n",
        "NULL_TOKEN = '<NULLTOKEN>'\n",
        "MAX_SENTENCE_LENGTH = 200\n",
        "\n",
        "\n",
        "class RottenTomatoesDataset(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        self.classes = [0,1,2,3,4] # sentiment scores\n",
        "        \n",
        "        raw_rows = [] # raw input\n",
        "        \n",
        "        with open('train.tsv') as tsvfile:\n",
        "            tlog('Loading training data...')\n",
        "            reader = csv.reader(tsvfile, delimiter='\\t')\n",
        "            count = 0\n",
        "            exceptions = 0\n",
        "            max_sentence = 0\n",
        "            for row in reader: # assuming sorted by sentenceid, phraseid\n",
        "                if count > 0: # skip header\n",
        "                    phraseID = int(row[I_PHRASE_ID])\n",
        "                    sentenceID = int(row[I_SENTENCE_ID])\n",
        "                    label = int(row[I_LABEL])\n",
        "                    if phraseID > 0 and sentenceID > 0 and label >= 0:\n",
        "                        # print(row)\n",
        "                        row[I_PHRASE_ID] = phraseID\n",
        "                        row[I_SENTENCE_ID] = sentenceID\n",
        "                        row[I_LABEL] = label\n",
        "                        raw_rows.append(row)\n",
        "                        max_sentence = max(max_sentence, sentenceID)\n",
        "                    else:\n",
        "                        print('EXCEPTION')\n",
        "                        print(row)\n",
        "                        exceptions += 1\n",
        "                count += 1\n",
        "            \n",
        "            \n",
        "            # gather tokens\n",
        "            (self.vocab_map, self.enriched_rows) = self.build_vocab_and_map_phrases(raw_rows)\n",
        "            \n",
        "            # break into training & validation\n",
        "            tlog('Splitting training and validation sets...')\n",
        "            i = 0\n",
        "            while self.enriched_rows[i][I_SENTENCE_ID] < (max_sentence * 0.8):\n",
        "                i += 1\n",
        "            self.training_rows, self.validation_rows = self.enriched_rows[:i], self.enriched_rows[i:]\n",
        "\n",
        "            # wrap it up\n",
        "            self.training = True\n",
        "            tlog('Finished loading training data:')\n",
        "            tlog('  {} exceptions in {} rows ({} good records)'.format(exceptions, count, count - exceptions))\n",
        "            tlog('  token count {}'.format(len(self.vocab_map)))\n",
        "\n",
        "    # helpers\n",
        "    def build_vocab_and_map_phrases(self, raw_rows):\n",
        "        tlog('Building vocabulary...')\n",
        "        vocab = set()\n",
        "        last_sentence_parsed = 0\n",
        "        for row in raw_rows:\n",
        "            sent_id = row[I_SENTENCE_ID]\n",
        "            if sent_id > last_sentence_parsed:\n",
        "                tokens = row[I_PHRASE].split(' ')\n",
        "                for token in tokens:\n",
        "                    vocab.add(token) # make them unique\n",
        "        vocab = list(vocab)\n",
        "        vocab.append(NULL_TOKEN)\n",
        "        vocab_map = {vocab[i]: i for i in range(len(vocab))}\n",
        "        \n",
        "        tlog('Mapping phrases to one-hot vectors...')\n",
        "        \n",
        "        enriched_rows = raw_rows\n",
        "        for i, row in enumerate(enriched_rows):\n",
        "            if i % 10000 == 0: tlog('  mapping row {} of {}'.format(i, len(raw_rows)))\n",
        "            token_list = []\n",
        "            tokens = row[I_PHRASE].split(' ')\n",
        "            for token in tokens:\n",
        "                if token in vocab:\n",
        "                    token_list.append(vocab_map[token])\n",
        "                else:\n",
        "                    token_list.append(vocab_map[NULL_TOKEN])\n",
        "            token_list = torch.tensor(token_list, dtype=torch.long)\n",
        "            padded_token_list = torch.zeros(MAX_SENTENCE_LENGTH, dtype=torch.long)\n",
        "            padded_token_list[:len(token_list)] = token_list\n",
        "            row.append(padded_token_list)\n",
        "            \n",
        "        tlog('Finished vocabulary mapping')\n",
        "        return vocab_map, enriched_rows\n",
        "    \n",
        "    \n",
        "    # two states, training and validation\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "    \n",
        "    def validate(self):\n",
        "        self.training = False\n",
        "    \n",
        "    def current_dataset(self):\n",
        "        if self.training:\n",
        "            return self.training_rows\n",
        "        return self.validation_rows\n",
        "\n",
        "    # the obligatory\n",
        "    def __len__(self):\n",
        "        return len(self.current_dataset())\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.current_dataset()[idx]\n",
        "        return row[I_TOKEN_LIST], row[I_LABEL]\n",
        "\n",
        "dataset = RottenTomatoesDataset()\n",
        "dataset.train()\n",
        "print(len(dataset))\n",
        "dataset.validate()\n",
        "print(len(dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDpAWgpFvBR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I've set up a single dataset class that (crudely) splits the set between training and validation sets with a roughly 80/20 split - see the cell below for usage."
      ]
    },
    {
      "metadata": {
        "id": "vixvL-G3ZJIT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The simple RNN-based network below transforms the input as follows:\n",
        "\n",
        "1) Initially, the input tensor has a one-hot vector with the same dimensionality as the dataset's total vocabulary. The Embedding layer converts this to a denser representation as a floating-point vector.\n",
        "\n",
        "2) The RNN layer maintains a hidden state that allows it to capture context from short sequences.\n",
        "\n",
        "3) Finally, the Linear layer classifies the phrase into one of our five sentiment classes (0-4)."
      ]
    },
    {
      "metadata": {
        "id": "3cTECxfUvSr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SentimentSeeker(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_classes):\n",
        "        super(SentimentSeeker, self).__init__()\n",
        "        \n",
        "        # at a minimum, should include:\n",
        "        # embedding layer\n",
        "        # RNN layer (or some RNN variant)\n",
        "        # linear layer (classifier)\n",
        "        \n",
        "    def forward(self, sen): # input vector of max sentence length containing one-hots\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}