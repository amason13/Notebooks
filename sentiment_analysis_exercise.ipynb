{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q-DoPGl0j7gc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k16HBOovsZ0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below are some helpers, and a few lines to nudge you to change your Colab runtime to GPU (just in case you hadn't already)."
      ]
    },
    {
      "metadata": {
        "id": "pHWwa-4JlM78",
        "colab_type": "code",
        "outputId": "bd358374-1190-41d3-e4a0-bfb9b9dd84e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def tlog(msg):\n",
        "    print('{}  {}'.format(time.asctime(), msg))\n",
        "\n",
        "\n",
        "# If possible, we should be running on GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print('If you are running this notebook in Colab, go to the Runtime menu and select \"Change runtime type\" to switch to GPU.')\n",
        "else:\n",
        "    print('GPU ready to go!')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MSjpzwUQsYyx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "teknD8kimZUz",
        "colab_type": "code",
        "outputId": "2bc028c8-6fbc-47bf-d27d-9bbead28176f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "# indices\n",
        "I_PHRASE_ID = 0\n",
        "I_SENTENCE_ID = 1\n",
        "I_PHRASE = 2\n",
        "I_LABEL = 3\n",
        "I_TOKEN_LIST = 4\n",
        "\n",
        "# constants\n",
        "NULL_TOKEN = '<NULLTOKEN>'\n",
        "MAX_SENTENCE_LENGTH = 200\n",
        "\n",
        "\n",
        "class RottenTomatoesDataset(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        self.classes = [0,1,2,3,4] # sentiment scores\n",
        "        \n",
        "        raw_rows = [] # raw input\n",
        "        \n",
        "        with open('train.tsv') as tsvfile:\n",
        "            tlog('Loading training data...')\n",
        "            reader = csv.reader(tsvfile, delimiter='\\t')\n",
        "            count = 0\n",
        "            exceptions = 0\n",
        "            max_sentence = 0\n",
        "            for row in reader: # assuming sorted by sentenceid, phraseid\n",
        "                if count > 0: # skip header\n",
        "                    phraseID = int(row[I_PHRASE_ID])\n",
        "                    sentenceID = int(row[I_SENTENCE_ID])\n",
        "                    label = int(row[I_LABEL])\n",
        "                    if phraseID > 0 and sentenceID > 0 and label >= 0:\n",
        "                        # print(row)\n",
        "                        row[I_PHRASE_ID] = phraseID\n",
        "                        row[I_SENTENCE_ID] = sentenceID\n",
        "                        row[I_LABEL] = label\n",
        "                        raw_rows.append(row)\n",
        "                        max_sentence = max(max_sentence, sentenceID)\n",
        "                    else:\n",
        "                        print('EXCEPTION')\n",
        "                        print(row)\n",
        "                        exceptions += 1\n",
        "                count += 1\n",
        "            \n",
        "            \n",
        "            # gather tokens\n",
        "            (self.vocab_map, self.enriched_rows) = self.build_vocab_and_map_phrases(raw_rows)\n",
        "            \n",
        "            # break into training & validation\n",
        "            tlog('Splitting training and validation sets...')\n",
        "            i = 0\n",
        "            while self.enriched_rows[i][I_SENTENCE_ID] < (max_sentence * 0.8):\n",
        "                i += 1\n",
        "            self.training_rows, self.validation_rows = self.enriched_rows[:i], self.enriched_rows[i:]\n",
        "\n",
        "            # wrap it up\n",
        "            self.training = True\n",
        "            tlog('Finished loading training data:')\n",
        "            tlog('  {} exceptions in {} rows ({} good records)'.format(exceptions, count, count - exceptions))\n",
        "            tlog('  token count {}'.format(len(self.vocab_map)))\n",
        "\n",
        "    # helpers\n",
        "    def build_vocab_and_map_phrases(self, raw_rows):\n",
        "        tlog('Building vocabulary...')\n",
        "        vocab = set()\n",
        "        last_sentence_parsed = 0\n",
        "        for row in raw_rows:\n",
        "            sent_id = row[I_SENTENCE_ID]\n",
        "            if sent_id > last_sentence_parsed:\n",
        "                tokens = row[I_PHRASE].split(' ')\n",
        "                for token in tokens:\n",
        "                    vocab.add(token) # make them unique\n",
        "        vocab = list(vocab)\n",
        "        vocab.append(NULL_TOKEN)\n",
        "        vocab_map = {vocab[i]: i for i in range(len(vocab))}\n",
        "        \n",
        "        tlog('Mapping phrases to one-hot vectors...')\n",
        "        \n",
        "        enriched_rows = raw_rows\n",
        "        for i, row in enumerate(enriched_rows):\n",
        "            if i % 10000 == 0: tlog('  mapping row {} of {}'.format(i, len(raw_rows)))\n",
        "            token_list = []\n",
        "            tokens = row[I_PHRASE].split(' ')\n",
        "            for token in tokens:\n",
        "                if token in vocab:\n",
        "                    token_list.append(vocab_map[token])\n",
        "                else:\n",
        "                    token_list.append(vocab_map[NULL_TOKEN])\n",
        "            token_list = torch.tensor(token_list, dtype=torch.long)\n",
        "            padded_token_list = torch.zeros(MAX_SENTENCE_LENGTH, dtype=torch.long)\n",
        "            padded_token_list[:len(token_list)] = token_list\n",
        "            row.append(padded_token_list)\n",
        "            \n",
        "        tlog('Finished vocabulary mapping')\n",
        "        return vocab_map, enriched_rows\n",
        "    \n",
        "    \n",
        "    # two states, training and validation\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "    \n",
        "    def validate(self):\n",
        "        self.training = False\n",
        "    \n",
        "    def current_dataset(self):\n",
        "        if self.training:\n",
        "            return self.training_rows\n",
        "        return self.validation_rows\n",
        "\n",
        "    # the obligatory\n",
        "    def __len__(self):\n",
        "        return len(self.current_dataset())\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.current_dataset()[idx]\n",
        "        return row[I_TOKEN_LIST], row[I_LABEL]\n",
        "\n",
        "dataset = RottenTomatoesDataset()\n",
        "dataset.train()\n",
        "print(len(dataset))\n",
        "dataset.validate()\n",
        "print(len(dataset))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 24 06:12:40 2019  Loading training data...\n",
            "Wed Apr 24 06:12:41 2019  Building vocabulary...\n",
            "Wed Apr 24 06:12:41 2019  Mapping phrases to one-hot vectors...\n",
            "Wed Apr 24 06:12:41 2019    mapping row 0 of 156060\n",
            "Wed Apr 24 06:12:49 2019    mapping row 10000 of 156060\n",
            "Wed Apr 24 06:12:58 2019    mapping row 20000 of 156060\n",
            "Wed Apr 24 06:13:07 2019    mapping row 30000 of 156060\n",
            "Wed Apr 24 06:13:17 2019    mapping row 40000 of 156060\n",
            "Wed Apr 24 06:13:26 2019    mapping row 50000 of 156060\n",
            "Wed Apr 24 06:13:36 2019    mapping row 60000 of 156060\n",
            "Wed Apr 24 06:13:46 2019    mapping row 70000 of 156060\n",
            "Wed Apr 24 06:13:56 2019    mapping row 80000 of 156060\n",
            "Wed Apr 24 06:14:06 2019    mapping row 90000 of 156060\n",
            "Wed Apr 24 06:14:16 2019    mapping row 100000 of 156060\n",
            "Wed Apr 24 06:14:26 2019    mapping row 110000 of 156060\n",
            "Wed Apr 24 06:14:37 2019    mapping row 120000 of 156060\n",
            "Wed Apr 24 06:14:47 2019    mapping row 130000 of 156060\n",
            "Wed Apr 24 06:14:58 2019    mapping row 140000 of 156060\n",
            "Wed Apr 24 06:15:08 2019    mapping row 150000 of 156060\n",
            "Wed Apr 24 06:15:15 2019  Finished vocabulary mapping\n",
            "Wed Apr 24 06:15:15 2019  Splitting training and validation sets...\n",
            "Wed Apr 24 06:15:15 2019  Finished loading training data:\n",
            "Wed Apr 24 06:15:15 2019    0 exceptions in 156061 rows (156061 good records)\n",
            "Wed Apr 24 06:15:15 2019    token count 18228\n",
            "127101\n",
            "28959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UDpAWgpFvBR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I've set up a single dataset class that (crudely) splits the set between training and validation sets with a roughly 80/20 split - see the cell below for usage."
      ]
    },
    {
      "metadata": {
        "id": "vixvL-G3ZJIT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The simple RNN-based network below transforms the input as follows:\n",
        "\n",
        "1) Initially, the input tensor has a one-hot vector with the same dimensionality as the dataset's total vocabulary. The Embedding layer converts this to a denser representation as a floating-point vector.\n",
        "\n",
        "2) The RNN layer maintains a hidden state that allows it to capture context from short sequences.\n",
        "\n",
        "3) Finally, the Linear layer classifies the phrase into one of our five sentiment classes (0-4)."
      ]
    },
    {
      "metadata": {
        "id": "3cTECxfUvSr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SentimentSeeker(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_classes):\n",
        "        super(SentimentSeeker, self).__init__()\n",
        "        \n",
        "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.classifier = torch.nn.Linear(hidden_dim, n_classes)\n",
        "        \n",
        "    def forward(self, sen): # input vector of max sentence length containing one-hots\n",
        "        # print(sen.shape)\n",
        "        embedded = self.embed(sen) # adds embedded dim\n",
        "        # print(embedded.shape)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # print(output.shape)\n",
        "        # print(hidden.shape)\n",
        "        x = self.classifier(hidden.squeeze(0))\n",
        "        # print(x.shape)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MK8BVxtdvUYX",
        "colab_type": "code",
        "outputId": "399a8f80-a8b4-4f7a-e872-6efda4874395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        }
      },
      "cell_type": "code",
      "source": [
        "# training constants\n",
        "N_EPOCHS = 2\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "dataset.train()\n",
        "train_loader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "model = SentimentSeeker(len(dataset.vocab_map), 32, 128, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(N_EPOCHS):\n",
        "    tlog('Epoch {} of {}'.format(epoch + 1, N_EPOCHS))\n",
        "    \n",
        "    dataset.train()\n",
        "    running_loss = 0.\n",
        "    \n",
        "    for batch_idx, (sens, labels) in enumerate(train_loader):\n",
        "        sens, labels = sens.to(device), labels.to(device)\n",
        "        sens.transpose_(0, 1)\n",
        "        model.zero_grad()\n",
        "        guesses = model(sens)\n",
        "        loss = F.cross_entropy(guesses, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx + 1) % 1000 == 0:\n",
        "            tlog('  batch {} of {}  avg loss per batch {}'.format(batch_idx + 1, len(dataset) // BATCH_SIZE, running_loss / 1000))\n",
        "            running_loss = 0.\n",
        "    tlog('Finished epoch {}'.format(epoch + 1))\n",
        "        \n",
        "    dataset.validate()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 24 06:43:52 2019  Epoch 1 of 2\n",
            "Wed Apr 24 06:44:00 2019    batch 1000 of 31775  avg loss per batch 1.2928992392718792\n",
            "Wed Apr 24 06:44:08 2019    batch 2000 of 31775  avg loss per batch 1.2783069897890091\n",
            "Wed Apr 24 06:44:15 2019    batch 3000 of 31775  avg loss per batch 1.3086948118209838\n",
            "Wed Apr 24 06:44:22 2019    batch 4000 of 31775  avg loss per batch 1.2836404772996903\n",
            "Wed Apr 24 06:44:29 2019    batch 5000 of 31775  avg loss per batch 1.2873399669528007\n",
            "Wed Apr 24 06:44:37 2019    batch 6000 of 31775  avg loss per batch 1.2962715511918068\n",
            "Wed Apr 24 06:44:44 2019    batch 7000 of 31775  avg loss per batch 1.2801318239569663\n",
            "Wed Apr 24 06:44:51 2019    batch 8000 of 31775  avg loss per batch 1.285779622554779\n",
            "Wed Apr 24 06:44:58 2019    batch 9000 of 31775  avg loss per batch 1.2849476900696755\n",
            "Wed Apr 24 06:45:05 2019    batch 10000 of 31775  avg loss per batch 1.287885831773281\n",
            "Wed Apr 24 06:45:13 2019    batch 11000 of 31775  avg loss per batch 1.2876967467665672\n",
            "Wed Apr 24 06:45:20 2019    batch 12000 of 31775  avg loss per batch 1.2901724135279655\n",
            "Wed Apr 24 06:45:28 2019    batch 13000 of 31775  avg loss per batch 1.2855017988085746\n",
            "Wed Apr 24 06:45:35 2019    batch 14000 of 31775  avg loss per batch 1.2491129227280617\n",
            "Wed Apr 24 06:45:42 2019    batch 15000 of 31775  avg loss per batch 1.284505469083786\n",
            "Wed Apr 24 06:45:50 2019    batch 16000 of 31775  avg loss per batch 1.283364480793476\n",
            "Wed Apr 24 06:45:57 2019    batch 17000 of 31775  avg loss per batch 1.2615341950058938\n",
            "Wed Apr 24 06:46:04 2019    batch 18000 of 31775  avg loss per batch 1.281427166044712\n",
            "Wed Apr 24 06:46:11 2019    batch 19000 of 31775  avg loss per batch 1.2675511370301247\n",
            "Wed Apr 24 06:46:18 2019    batch 20000 of 31775  avg loss per batch 1.269879248380661\n",
            "Wed Apr 24 06:46:25 2019    batch 21000 of 31775  avg loss per batch 1.282465699493885\n",
            "Wed Apr 24 06:46:32 2019    batch 22000 of 31775  avg loss per batch 1.30279400151968\n",
            "Wed Apr 24 06:46:40 2019    batch 23000 of 31775  avg loss per batch 1.276933068215847\n",
            "Wed Apr 24 06:46:48 2019    batch 24000 of 31775  avg loss per batch 1.2928792207241058\n",
            "Wed Apr 24 06:46:55 2019    batch 25000 of 31775  avg loss per batch 1.2925988991856574\n",
            "Wed Apr 24 06:47:02 2019    batch 26000 of 31775  avg loss per batch 1.275603584110737\n",
            "Wed Apr 24 06:47:09 2019    batch 27000 of 31775  avg loss per batch 1.2977709602117538\n",
            "Wed Apr 24 06:47:16 2019    batch 28000 of 31775  avg loss per batch 1.2922051177620888\n",
            "Wed Apr 24 06:47:23 2019    batch 29000 of 31775  avg loss per batch 1.2701200819015503\n",
            "Wed Apr 24 06:47:30 2019    batch 30000 of 31775  avg loss per batch 1.2800503155589105\n",
            "Wed Apr 24 06:47:38 2019    batch 31000 of 31775  avg loss per batch 1.3029947691559791\n",
            "Wed Apr 24 06:47:44 2019  Finished epoch 1\n",
            "Wed Apr 24 06:47:44 2019  Epoch 2 of 2\n",
            "Wed Apr 24 06:47:52 2019    batch 1000 of 31775  avg loss per batch 1.2803011860847473\n",
            "Wed Apr 24 06:47:59 2019    batch 2000 of 31775  avg loss per batch 1.2774385056495667\n",
            "Wed Apr 24 06:48:07 2019    batch 3000 of 31775  avg loss per batch 1.284648552775383\n",
            "Wed Apr 24 06:48:14 2019    batch 4000 of 31775  avg loss per batch 1.2680995749235153\n",
            "Wed Apr 24 06:48:21 2019    batch 5000 of 31775  avg loss per batch 1.300179309308529\n",
            "Wed Apr 24 06:48:28 2019    batch 6000 of 31775  avg loss per batch 1.2813952792882919\n",
            "Wed Apr 24 06:48:35 2019    batch 7000 of 31775  avg loss per batch 1.2887563822865487\n",
            "Wed Apr 24 06:48:42 2019    batch 8000 of 31775  avg loss per batch 1.279924535393715\n",
            "Wed Apr 24 06:48:50 2019    batch 9000 of 31775  avg loss per batch 1.3027940146923065\n",
            "Wed Apr 24 06:48:57 2019    batch 10000 of 31775  avg loss per batch 1.29224962246418\n",
            "Wed Apr 24 06:49:04 2019    batch 11000 of 31775  avg loss per batch 1.275498637199402\n",
            "Wed Apr 24 06:49:11 2019    batch 12000 of 31775  avg loss per batch 1.2833607047200204\n",
            "Wed Apr 24 06:49:18 2019    batch 13000 of 31775  avg loss per batch 1.2665773773789406\n",
            "Wed Apr 24 06:49:26 2019    batch 14000 of 31775  avg loss per batch 1.2707610076665878\n",
            "Wed Apr 24 06:49:33 2019    batch 15000 of 31775  avg loss per batch 1.275947531938553\n",
            "Wed Apr 24 06:49:40 2019    batch 16000 of 31775  avg loss per batch 1.2631765590906143\n",
            "Wed Apr 24 06:49:48 2019    batch 17000 of 31775  avg loss per batch 1.285863475561142\n",
            "Wed Apr 24 06:49:55 2019    batch 18000 of 31775  avg loss per batch 1.2869670533537865\n",
            "Wed Apr 24 06:50:02 2019    batch 19000 of 31775  avg loss per batch 1.2723515216112138\n",
            "Wed Apr 24 06:50:09 2019    batch 20000 of 31775  avg loss per batch 1.2748982567191125\n",
            "Wed Apr 24 06:50:16 2019    batch 21000 of 31775  avg loss per batch 1.2754593484401704\n",
            "Wed Apr 24 06:50:23 2019    batch 22000 of 31775  avg loss per batch 1.2751352038383483\n",
            "Wed Apr 24 06:50:31 2019    batch 23000 of 31775  avg loss per batch 1.2865874525904655\n",
            "Wed Apr 24 06:50:38 2019    batch 24000 of 31775  avg loss per batch 1.289443806529045\n",
            "Wed Apr 24 06:50:46 2019    batch 25000 of 31775  avg loss per batch 1.2703143811225892\n",
            "Wed Apr 24 06:50:53 2019    batch 26000 of 31775  avg loss per batch 1.2868809201717377\n",
            "Wed Apr 24 06:51:00 2019    batch 27000 of 31775  avg loss per batch 1.2926509379148483\n",
            "Wed Apr 24 06:51:08 2019    batch 28000 of 31775  avg loss per batch 1.2937265110611915\n",
            "Wed Apr 24 06:51:15 2019    batch 29000 of 31775  avg loss per batch 1.2753117989897729\n",
            "Wed Apr 24 06:51:22 2019    batch 30000 of 31775  avg loss per batch 1.287525588274002\n",
            "Wed Apr 24 06:51:29 2019    batch 31000 of 31775  avg loss per batch 1.281505268752575\n",
            "Wed Apr 24 06:51:34 2019  Finished epoch 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "92X0ZknH8V8K",
        "colab_type": "code",
        "outputId": "a2905964-2a22-40c0-fba5-ea7abe9892ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28959\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}